{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MVP Siamese LSTM Net\n",
    "\n",
    "This is a baseline siamese LSTM net. The purpose is to build out the architecture, and see if the net can get as good as validation score as the classifiers.\n",
    "\n",
    "Ideas Implemented:\n",
    "* Increased Dropout rate to 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:05:40.872824Z",
     "start_time": "2018-11-29T00:05:40.835246Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# data manipulation\n",
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Input, Add, concatenate, BatchNormalization\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# plotting\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:54:40.060662Z",
     "start_time": "2018-11-28T23:54:39.617510Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = utils.load('X_train')\n",
    "y_train = utils.load('y_train')\n",
    "model_name = 'lstm_dropout_50'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Encode vocabulary\n",
    "\n",
    "1. Limit the vocab to 20,000 words.\n",
    "2. Clean questions only and do not lemmatize.\n",
    "3. Limit the question length to 100 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:54:48.602373Z",
     "start_time": "2018-11-28T23:54:40.063608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(606398, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7212"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = 20000\n",
    "max_q_len = 100\n",
    "\n",
    "X_train_stack = utils.clean_questions(utils.stack_questions(X_train))\n",
    "\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(X_train_stack)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(X_train_stack)\n",
    "data = pad_sequences(sequences, maxlen=max_q_len)\n",
    "\n",
    "print(data.shape)\n",
    "data[:,0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Matrix\n",
    "\n",
    "1. Calculates the embedding matrix utilizing spaCy `en_core_web_lg` word vectors.\n",
    "  * https://spacy.io/models/en#en_core_web_lg\n",
    "  * GloVe vectors trained on Common Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:59:00.965247Z",
     "start_time": "2018-11-28T23:55:20.356138Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    embedding_matrix = utils.load('embedding_matrix')\n",
    "except:\n",
    "    # create a weight matrix for words in training docs\n",
    "    embedding_matrix = np.zeros((vocabulary_size, 300))\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "    #     print(word, index, end='\\r')\n",
    "        if index > vocabulary_size - 1:\n",
    "            break\n",
    "        else:\n",
    "            embedding_vector = utils.nlp(word).vector\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[index] = embedding_vector\n",
    "    #     break\n",
    "\n",
    "    utils.save(embedding_matrix, 'embedding_matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the batch to pass into the network\n",
    "\n",
    "Create arrays to split the stacked data into question 1 set and question 2 set for each pair.\n",
    "\n",
    "**Need to cleanup this cell in the next model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:25:57.617365Z",
     "start_time": "2018-11-29T00:25:57.429291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train major class: 0.63\n",
      "Val major class: 0.63\n"
     ]
    }
   ],
   "source": [
    "# cooncatenate the two questions\n",
    "odd_idx = [i for i in range(data.shape[0]) if i % 2 == 1]\n",
    "even_idx = [i for i in range(data.shape[0]) if i % 2 == 0]\n",
    "\n",
    "data_1 = data[odd_idx]\n",
    "data_2 = data[odd_idx]\n",
    "\n",
    "# split the data set into a validation set\n",
    "data_train, data_val, label_train, label_val = train_test_split(np.hstack([data_1, data_2]), \n",
    "                                                                y_train, \n",
    "                                                                stratify=y_train, \n",
    "                                                                test_size = 0.33,\n",
    "                                                                random_state=42)\n",
    "\n",
    "# split the concatenation back into 2 data sets for the siamese network\n",
    "data_1_train = data_train[:, :max_q_len]\n",
    "data_2_train = data_train[:, max_q_len:]\n",
    "data_1_val = data_val[:, :max_q_len]\n",
    "data_2_val = data_val[:, max_q_len:]\n",
    "\n",
    "print(f'Train major class: {len(label_train[label_train == 0]) / len(label_train):.2}')\n",
    "print(f'Val major class: {len(label_val[label_val == 0]) / len(label_val):.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build out legs of the siamese network\n",
    "\n",
    "The architecure is the following,\n",
    "\n",
    "0. Input - (100,) word tensor\n",
    "1. Embedding Layer - outputs (300,) **not trainable**\n",
    "2. LSTM - default outputs (300,)\n",
    "3. Concatenate the two nets outputs (600,)\n",
    "4. BatchNormalization\n",
    "5. Dropout - 20%\n",
    "6. Dense - outputs (100,), activation `tanh` -- somewhat random decision\n",
    "7. BatchNormalization\n",
    "8. Dropout - 20%\n",
    "9. Dense - outputs (1,), activation `sigmoid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:15:21.951772Z",
     "start_time": "2018-11-29T00:15:19.924658Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating word embedding layer\n",
    "embedding_layer = Embedding(vocabulary_size, 300, input_length=100, \n",
    "                                     weights=[embedding_matrix], trainable=False)\n",
    "\n",
    "# Creating LSTM Encoder\n",
    "# Bidirectional(LSTM(self.number_lstm_units, dropout=self.rate_drop_lstm, recurrent_dropout=self.rate_drop_lstm))\n",
    "lstm_layer = LSTM(300)\n",
    "\n",
    "# Creating LSTM Encoder layer for First Sentence\n",
    "sequence_1_input = Input(shape=(100,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "# Creating LSTM Encoder layer for Second Sentence\n",
    "sequence_2_input = Input(shape=(100,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "x2 = lstm_layer(embedded_sequences_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:16:12.624956Z",
     "start_time": "2018-11-29T00:16:12.372819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 300)     6000000     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 300)          721200      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 600)          0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 600)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          60100       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            101         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,781,401\n",
      "Trainable params: 781,401\n",
      "Non-trainable params: 6,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Merging two LSTM encodes vectors from sentences to\n",
    "# pass it to dense layer applying dropout and batch normalisation\n",
    "\n",
    "merged = concatenate([x1, x2])\n",
    "# merged = BatchNormalization()(merged)\n",
    "merged = Dropout(.5)(merged)\n",
    "merged = Dense(100, activation='tanh')(merged)\n",
    "# merged = BatchNormalization()(merged)\n",
    "merged = Dropout(0.5)(merged)\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "# SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "\n",
    "file_path = '../data/keras_models/' + model_name + '_{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(filepath=file_path, save_best_only=True)\n",
    "\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='../data/tensorboard')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                               min_delta=0, \n",
    "                               patience=3, \n",
    "                               verbose=1, \n",
    "                               mode='auto', \n",
    "                               restore_best_weights=True)\n",
    "\n",
    "# calc_auc = IntervalEvaluation(([data_1_val, data_2_val], label_val), interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-29T00:26:39.525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203143 samples, validate on 100056 samples\n",
      "Epoch 1/200\n",
      "203143/203143 [==============================] - 557s 3ms/step - loss: 0.5740 - acc: 0.7015 - val_loss: 0.5723 - val_acc: 0.7130\n",
      "Epoch 2/200\n",
      "203143/203143 [==============================] - 556s 3ms/step - loss: 0.5301 - acc: 0.7373 - val_loss: 0.5376 - val_acc: 0.7305\n",
      "Epoch 3/200\n",
      "203143/203143 [==============================] - 553s 3ms/step - loss: 0.5022 - acc: 0.7566 - val_loss: 0.5267 - val_acc: 0.7454\n",
      "Epoch 4/200\n",
      "203143/203143 [==============================] - 543s 3ms/step - loss: 0.4715 - acc: 0.7770 - val_loss: 0.5229 - val_acc: 0.7505\n",
      "Epoch 5/200\n",
      "203143/203143 [==============================] - 542s 3ms/step - loss: 0.4348 - acc: 0.8000 - val_loss: 0.5218 - val_acc: 0.7518\n",
      "Epoch 6/200\n",
      "203143/203143 [==============================] - 541s 3ms/step - loss: 0.3959 - acc: 0.8229 - val_loss: 0.5421 - val_acc: 0.7517\n",
      "Epoch 7/200\n",
      "203143/203143 [==============================] - 542s 3ms/step - loss: 0.3586 - acc: 0.8433 - val_loss: 0.5684 - val_acc: 0.7507\n",
      "Epoch 8/200\n",
      "203143/203143 [==============================] - 541s 3ms/step - loss: 0.3240 - acc: 0.8630 - val_loss: 0.6207 - val_acc: 0.7352\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00008: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f52d94a90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([data_1_train, data_2_train], label_train, \n",
    "          validation_data=([data_1_val, data_2_val], label_val),\n",
    "                  epochs=200, batch_size=128, shuffle=True,\n",
    "                  callbacks=[model_checkpoint, tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100056/100056 [==============================] - 88s 877us/step\n"
     ]
    }
   ],
   "source": [
    "# model = load_model('../data/keras_models/mvp_batch_norm08-0.54.hdf5')\n",
    "\n",
    "y_prob = model.predict([data_1_val, data_2_val], batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_accuracy</th>\n",
       "      <th>std_accuracy</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>std_precision</th>\n",
       "      <th>avg_recall</th>\n",
       "      <th>std_recall</th>\n",
       "      <th>avg_f1</th>\n",
       "      <th>std_f1</th>\n",
       "      <th>avg_auc</th>\n",
       "      <th>std_auc</th>\n",
       "      <th>avg_log_loss</th>\n",
       "      <th>std_log_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rf_feat_eng_model_lemma_clean</th>\n",
       "      <td>0.783667</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>0.708853</td>\n",
       "      <td>0.003681</td>\n",
       "      <td>0.702725</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>0.705774</td>\n",
       "      <td>0.002658</td>\n",
       "      <td>0.868202</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.436197</td>\n",
       "      <td>0.000640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensemble_rf_xgb</th>\n",
       "      <td>0.779000</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.697794</td>\n",
       "      <td>0.004357</td>\n",
       "      <td>0.708157</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.702935</td>\n",
       "      <td>0.003148</td>\n",
       "      <td>0.863334</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.441784</td>\n",
       "      <td>0.001107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_eng_model_lemma_clean</th>\n",
       "      <td>0.763927</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.676166</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>0.692113</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.684044</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.846923</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>0.456929</td>\n",
       "      <td>0.001410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_eng_model_lemma_fix</th>\n",
       "      <td>0.744356</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>0.664513</td>\n",
       "      <td>0.004333</td>\n",
       "      <td>0.621357</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.642201</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>0.822197</td>\n",
       "      <td>0.001710</td>\n",
       "      <td>0.488131</td>\n",
       "      <td>0.001342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_eng_model</th>\n",
       "      <td>0.743614</td>\n",
       "      <td>0.002021</td>\n",
       "      <td>0.664102</td>\n",
       "      <td>0.003502</td>\n",
       "      <td>0.618400</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>0.640434</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>0.821070</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.489465</td>\n",
       "      <td>0.001141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensemble_rf_xgb_cos_sim</th>\n",
       "      <td>0.738700</td>\n",
       "      <td>0.007359</td>\n",
       "      <td>0.661290</td>\n",
       "      <td>0.010948</td>\n",
       "      <td>0.612827</td>\n",
       "      <td>0.009669</td>\n",
       "      <td>0.636128</td>\n",
       "      <td>0.009994</td>\n",
       "      <td>0.819987</td>\n",
       "      <td>0.005193</td>\n",
       "      <td>0.493703</td>\n",
       "      <td>0.003901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_dropout_50</th>\n",
       "      <td>0.751849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.690400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.594510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.638877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.802315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.570912</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_mvp</th>\n",
       "      <td>0.749760</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.685627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.595133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.637183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.801019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.643059</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cos_sim_tfidf_model</th>\n",
       "      <td>0.729511</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.661680</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.547188</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.599010</td>\n",
       "      <td>0.001703</td>\n",
       "      <td>0.800271</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>0.512085</td>\n",
       "      <td>0.001299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_dropout50_dense50_BatchNorm</th>\n",
       "      <td>0.728612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.682992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.573654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.772344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.373478</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cos_sim_model</th>\n",
       "      <td>0.710200</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.658748</td>\n",
       "      <td>0.002578</td>\n",
       "      <td>0.446336</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>0.532120</td>\n",
       "      <td>0.001306</td>\n",
       "      <td>0.746769</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.565250</td>\n",
       "      <td>0.000963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mvp (tf-idf, nmf(5), xgboost)</th>\n",
       "      <td>0.700345</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.661571</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.385736</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>0.487325</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.740593</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.568958</td>\n",
       "      <td>0.001288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mvp (+ lemma)</th>\n",
       "      <td>0.696787</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.649977</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.387424</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>0.485464</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>0.738037</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.572483</td>\n",
       "      <td>0.000815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  avg_accuracy  std_accuracy  avg_precision  \\\n",
       "rf_feat_eng_model_lemma_clean         0.783667      0.002260       0.708853   \n",
       "ensemble_rf_xgb                       0.779000      0.002740       0.697794   \n",
       "feat_eng_model_lemma_clean            0.763927      0.002404       0.676166   \n",
       "feat_eng_model_lemma_fix              0.744356      0.002107       0.664513   \n",
       "feat_eng_model                        0.743614      0.002021       0.664102   \n",
       "ensemble_rf_xgb_cos_sim               0.738700      0.007359       0.661290   \n",
       "lstm_dropout_50                       0.751849      0.000000       0.690400   \n",
       "lstm_mvp                              0.749760      0.000000       0.685627   \n",
       "cos_sim_tfidf_model                   0.729511      0.001216       0.661680   \n",
       "lstm_dropout50_dense50_BatchNorm      0.728612      0.000000       0.682992   \n",
       "cos_sim_model                         0.710200      0.000830       0.658748   \n",
       "mvp (tf-idf, nmf(5), xgboost)         0.700345      0.000466       0.661571   \n",
       "mvp (+ lemma)                         0.696787      0.001055       0.649977   \n",
       "\n",
       "                                  std_precision  avg_recall  std_recall  \\\n",
       "rf_feat_eng_model_lemma_clean          0.003681    0.702725    0.001666   \n",
       "ensemble_rf_xgb                        0.004357    0.708157    0.001912   \n",
       "feat_eng_model_lemma_clean             0.003904    0.692113    0.001128   \n",
       "feat_eng_model_lemma_fix               0.004333    0.621357    0.000901   \n",
       "feat_eng_model                         0.003502    0.618400    0.001553   \n",
       "ensemble_rf_xgb_cos_sim                0.010948    0.612827    0.009669   \n",
       "lstm_dropout_50                        0.000000    0.594510    0.000000   \n",
       "lstm_mvp                               0.000000    0.595133    0.000000   \n",
       "cos_sim_tfidf_model                    0.002219    0.547188    0.001744   \n",
       "lstm_dropout50_dense50_BatchNorm       0.000000    0.494492    0.000000   \n",
       "cos_sim_model                          0.002578    0.446336    0.002215   \n",
       "mvp (tf-idf, nmf(5), xgboost)          0.000461    0.385736    0.002493   \n",
       "mvp (+ lemma)                          0.003057    0.387424    0.003230   \n",
       "\n",
       "                                    avg_f1    std_f1   avg_auc   std_auc  \\\n",
       "rf_feat_eng_model_lemma_clean     0.705774  0.002658  0.868202  0.001148   \n",
       "ensemble_rf_xgb                   0.702935  0.003148  0.863334  0.001438   \n",
       "feat_eng_model_lemma_clean        0.684044  0.002549  0.846923  0.001643   \n",
       "feat_eng_model_lemma_fix          0.642201  0.001609  0.822197  0.001710   \n",
       "feat_eng_model                    0.640434  0.002281  0.821070  0.001428   \n",
       "ensemble_rf_xgb_cos_sim           0.636128  0.009994  0.819987  0.005193   \n",
       "lstm_dropout_50                   0.638877  0.000000  0.802315  0.000000   \n",
       "lstm_mvp                          0.637183  0.000000  0.801019  0.000000   \n",
       "cos_sim_tfidf_model               0.599010  0.001703  0.800271  0.001291   \n",
       "lstm_dropout50_dense50_BatchNorm  0.573654  0.000000  0.772344  0.000000   \n",
       "cos_sim_model                     0.532120  0.001306  0.746769  0.001279   \n",
       "mvp (tf-idf, nmf(5), xgboost)     0.487325  0.001983  0.740593  0.001647   \n",
       "mvp (+ lemma)                     0.485464  0.002485  0.738037  0.001362   \n",
       "\n",
       "                                  avg_log_loss  std_log_loss  \n",
       "rf_feat_eng_model_lemma_clean         0.436197      0.000640  \n",
       "ensemble_rf_xgb                       0.441784      0.001107  \n",
       "feat_eng_model_lemma_clean            0.456929      0.001410  \n",
       "feat_eng_model_lemma_fix              0.488131      0.001342  \n",
       "feat_eng_model                        0.489465      0.001141  \n",
       "ensemble_rf_xgb_cos_sim               0.493703      0.003901  \n",
       "lstm_dropout_50                       8.570912      0.000000  \n",
       "lstm_mvp                              8.643059      0.000000  \n",
       "cos_sim_tfidf_model                   0.512085      0.001299  \n",
       "lstm_dropout50_dense50_BatchNorm      9.373478      0.000000  \n",
       "cos_sim_model                         0.565250      0.000963  \n",
       "mvp (tf-idf, nmf(5), xgboost)         0.568958      0.001288  \n",
       "mvp (+ lemma)                         0.572483      0.000815  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = utils.load('results')\n",
    "\n",
    "results_df = results_df.drop(index=model_name, errors='ignore')\n",
    "results_df = results_df.append(utils.log_keras_scores(label_val, y_prob, model_name))\n",
    "results_df.sort_values('avg_auc', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save(results_df, 'results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "Slightly better. Let's now apply a dropout rates to the LSTM layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
