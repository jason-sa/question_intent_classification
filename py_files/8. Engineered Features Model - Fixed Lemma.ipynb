{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced featrue engineer model\n",
    "\n",
    "Engineered two different types of features,\n",
    "\n",
    "1. n_gram similarity between each pair of questions\n",
    "2. min/max/avg distance between words in a single question. Currently using the following metrics,\n",
    "  * euclidean\n",
    "  * cosine\n",
    "  * city block or manhattan\n",
    "  \n",
    "**Pipeline**\n",
    "1. Stack questions\n",
    "2. Clean questions - now lower cases all words to better lemmatize proper nouns\n",
    "3. Lemmatize questions\n",
    "4. UNION\n",
    "    1. n_gram similarity\n",
    "    2. min/max/avg distances\n",
    "5. XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T03:51:59.835914Z",
     "start_time": "2018-11-28T03:51:40.783033Z"
    }
   },
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# modeling\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T03:52:00.522066Z",
     "start_time": "2018-11-28T03:51:59.853974Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = utils.load('X_train')\n",
    "y_train = utils.load('y_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T03:55:13.902055Z",
     "start_time": "2018-11-28T03:55:13.884969Z"
    }
   },
   "outputs": [],
   "source": [
    "single_question_pipe = Pipeline(\n",
    "    [\n",
    "        ('dist', FunctionTransformer(utils.add_min_max_avg_distance_features, validate=False)),\n",
    "        ('unstack', FunctionTransformer(utils.unstack_questions, validate=False))\n",
    "    ]\n",
    ")\n",
    "\n",
    "pre_process_pipe = Pipeline(\n",
    "    [\n",
    "        ('stack', FunctionTransformer(utils.stack_questions, validate=False)),\n",
    "        ('clean', FunctionTransformer(utils.clean_questions, validate=False)),\n",
    "        ('lemma', FunctionTransformer(utils.apply_lemma, validate=False)),\n",
    "        ('feats', FeatureUnion(\n",
    "            [\n",
    "                ('ngram_sim', FunctionTransformer(utils.calc_ngram_similarity, kw_args={'n_grams':[1, 2, 3]}, validate=False)),\n",
    "                ('single', single_question_pipe)\n",
    "            ]\n",
    "        ))\n",
    "    ]\n",
    ")\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=500, n_jobs=-1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T04:02:31.374857Z",
     "start_time": "2018-11-28T03:55:15.482152Z"
    }
   },
   "outputs": [],
   "source": [
    "X_transform = pre_process_pipe.transform(X_train)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3, random_state=42)\n",
    "cv = cross_validate(xgb, \n",
    "               X_transform, \n",
    "               y_train, \n",
    "               cv=skf, \n",
    "               n_jobs=-1, \n",
    "               scoring=('accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'neg_log_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_accuracy</th>\n",
       "      <th>std_accuracy</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>std_precision</th>\n",
       "      <th>avg_recall</th>\n",
       "      <th>std_recall</th>\n",
       "      <th>avg_f1</th>\n",
       "      <th>std_f1</th>\n",
       "      <th>avg_auc</th>\n",
       "      <th>std_auc</th>\n",
       "      <th>avg_log_loss</th>\n",
       "      <th>std_log_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mvp (tf-idf, nmf(5), xgboost)</th>\n",
       "      <td>0.700345</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.661571</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.385736</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>0.487325</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.740593</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.568958</td>\n",
       "      <td>0.001288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mvp (+ lemma)</th>\n",
       "      <td>0.696787</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.649977</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.387424</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>0.485464</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>0.738037</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.572483</td>\n",
       "      <td>0.000815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cos_sim_model</th>\n",
       "      <td>0.710200</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.658748</td>\n",
       "      <td>0.002578</td>\n",
       "      <td>0.446336</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>0.532120</td>\n",
       "      <td>0.001306</td>\n",
       "      <td>0.746769</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.565250</td>\n",
       "      <td>0.000963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cos_sim_tfidf_model</th>\n",
       "      <td>0.728261</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>0.659662</td>\n",
       "      <td>0.002240</td>\n",
       "      <td>0.545419</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.597124</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>0.799173</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>0.513172</td>\n",
       "      <td>0.001191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_eng_model</th>\n",
       "      <td>0.743614</td>\n",
       "      <td>0.002021</td>\n",
       "      <td>0.664102</td>\n",
       "      <td>0.003502</td>\n",
       "      <td>0.618400</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>0.640434</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>0.821070</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.489465</td>\n",
       "      <td>0.001141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_eng_model_lemma_fix</th>\n",
       "      <td>0.744356</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>0.664513</td>\n",
       "      <td>0.004333</td>\n",
       "      <td>0.621357</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.642201</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>0.822197</td>\n",
       "      <td>0.001710</td>\n",
       "      <td>0.488131</td>\n",
       "      <td>0.001342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               avg_accuracy  std_accuracy  avg_precision  \\\n",
       "mvp (tf-idf, nmf(5), xgboost)      0.700345      0.000466       0.661571   \n",
       "mvp (+ lemma)                      0.696787      0.001055       0.649977   \n",
       "cos_sim_model                      0.710200      0.000830       0.658748   \n",
       "cos_sim_tfidf_model                0.728261      0.001248       0.659662   \n",
       "feat_eng_model                     0.743614      0.002021       0.664102   \n",
       "feat_eng_model_lemma_fix           0.744356      0.002107       0.664513   \n",
       "\n",
       "                               std_precision  avg_recall  std_recall  \\\n",
       "mvp (tf-idf, nmf(5), xgboost)       0.000461    0.385736    0.002493   \n",
       "mvp (+ lemma)                       0.003057    0.387424    0.003230   \n",
       "cos_sim_model                       0.002578    0.446336    0.002215   \n",
       "cos_sim_tfidf_model                 0.002240    0.545419    0.001370   \n",
       "feat_eng_model                      0.003502    0.618400    0.001553   \n",
       "feat_eng_model_lemma_fix            0.004333    0.621357    0.000901   \n",
       "\n",
       "                                 avg_f1    std_f1   avg_auc   std_auc  \\\n",
       "mvp (tf-idf, nmf(5), xgboost)  0.487325  0.001983  0.740593  0.001647   \n",
       "mvp (+ lemma)                  0.485464  0.002485  0.738037  0.001362   \n",
       "cos_sim_model                  0.532120  0.001306  0.746769  0.001279   \n",
       "cos_sim_tfidf_model            0.597124  0.001666  0.799173  0.001407   \n",
       "feat_eng_model                 0.640434  0.002281  0.821070  0.001428   \n",
       "feat_eng_model_lemma_fix       0.642201  0.001609  0.822197  0.001710   \n",
       "\n",
       "                               avg_log_loss  std_log_loss  \n",
       "mvp (tf-idf, nmf(5), xgboost)      0.568958      0.001288  \n",
       "mvp (+ lemma)                      0.572483      0.000815  \n",
       "cos_sim_model                      0.565250      0.000963  \n",
       "cos_sim_tfidf_model                0.513172      0.001191  \n",
       "feat_eng_model                     0.489465      0.001141  \n",
       "feat_eng_model_lemma_fix           0.488131      0.001342  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = utils.load('results')\n",
    "\n",
    "results_df = results_df.drop(index='feat_eng_model_lemma_fix', errors='ignore')\n",
    "results_df = results_df.append(utils.log_scores(cv, 'feat_eng_model_lemma_fix'))\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save(results_df, 'results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Wow! The feature engineering shows a significant jump in AUC from 0.8 to 0.82."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gt</th>\n",
       "      <th>prob</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.376427</td>\n",
       "      <td>-0.376427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.498056</td>\n",
       "      <td>-0.498056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306907</td>\n",
       "      <td>-0.306907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.008354</td>\n",
       "      <td>-0.008354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.020841</td>\n",
       "      <td>-0.020841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gt      prob      diff\n",
       "0   0  0.376427 -0.376427\n",
       "1   0  0.498056 -0.498056\n",
       "2   0  0.306907 -0.306907\n",
       "3   0  0.008354 -0.008354\n",
       "4   0  0.020841 -0.020841"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.fit(X_transform, y_train)\n",
    "y_probs = xgb.predict_proba(X_transform)[:, 1]\n",
    "class_errors_df = utils.ground_truth_analysis(y_train, y_probs)\n",
    "class_errors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_pipe = Pipeline(\n",
    "    [\n",
    "        ('stack', FunctionTransformer(utils.stack_questions, validate=False)),\n",
    "        ('clean', FunctionTransformer(utils.clean_questions, validate=False)),\n",
    "        ('lemma', FunctionTransformer(utils.apply_lemma, validate=False)),\n",
    "    ]\n",
    ")\n",
    "X_train_lemma = lemma_pipe.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top false negative errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob: 0.0038068118\n",
      "\n",
      "Is it right that males are more sexual than females?\n",
      "Do men have more sex drive than women?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "man sex drive woman\n",
      "right male sexual female\n",
      "\n",
      "Feature Space------\n",
      "[0.00000000e+00 0.00000000e+00 0.00000000e+00 2.65893855e+00\n",
      " 8.09540605e+00 6.68566402e+00 6.57226864e-02 7.81242170e-01\n",
      " 5.30844962e-01 3.68800914e+01 1.10758113e+02 9.17549343e+01\n",
      " 4.80341970e+00 9.19854137e+00 7.41691832e+00 2.59825547e-01\n",
      " 7.92403629e-01 5.83720550e-01 6.58726917e+01 1.27245393e+02\n",
      " 1.01846836e+02]\n",
      "-------------------------------------------\n",
      "\n",
      "Prob: 0.0038958814\n",
      "\n",
      "Is it possible that everyone else is just an imitation of consciousness, and I am the only real conscious being in existence? What would this imply?\n",
      "How do you prove that I am not the only person in the Matrix and everyone else is just a computer program?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "prove person matrix computer program\n",
      "possible imitation consciousness real conscious existence imply\n",
      "\n",
      "Feature Space------\n",
      "[  0.           0.           0.           4.71883175   8.39991169\n",
      "   6.39053384   0.25638661   0.82662229   0.59882083  64.79432624\n",
      " 117.34173503  88.3337425    6.0744299    8.91076416   7.73634829\n",
      "   0.53397435   0.90404939   0.72709112  82.43533646 115.84643379\n",
      " 103.62282989]\n",
      "-------------------------------------------\n",
      "\n",
      "Prob: 0.004106611\n",
      "\n",
      "Why do Indians care so much about what others think about them?\n",
      "Why are Indians so obsessed in knowing thoughts of other countries about them?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "indian care think\n",
      "indians obsessed know thought country\n",
      "\n",
      "Feature Space------\n",
      "[  0.           0.           0.           3.25350506   8.52300503\n",
      "   6.63022393   0.20168926   0.84591079   0.63122248  45.8899585\n",
      " 117.52870856  90.46288651   6.3029921    9.03850196   7.94280888\n",
      "   0.56617151   0.83719242   0.74641349  85.28026848 122.26219524\n",
      " 107.73483232]\n",
      "-------------------------------------------\n",
      "\n",
      "Prob: 0.004132414\n",
      "\n",
      "I broke up with my boyfriend but I still miss him and sometimes I feel like speaking to him and telling him to give this relation one more chance. What should I do?\n",
      "What do I do? We broke up, I want to ring or text him but I know I shouldn't I still have feelings for & miss him immensely?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "broke want ring text know feeling  miss immensely\n",
      "break boyfriend miss feel like speak tell relation chance\n",
      "\n",
      "Feature Space------\n",
      "[1.11111111e-01 0.00000000e+00 0.00000000e+00 4.26881063e+00\n",
      " 7.81655732e+00 5.87477796e+00 3.14317119e-01 9.01154566e-01\n",
      " 5.69704441e-01 5.89014515e+01 1.02208701e+02 7.95972512e+01\n",
      " 3.01465977e+00 8.47404180e+00 6.63696641e+00 1.68658416e-01\n",
      " 8.96998579e-01 6.64675291e-01 4.14176240e+01 1.15995651e+02\n",
      " 9.03136736e+01]\n",
      "-------------------------------------------\n",
      "\n",
      "Prob: 0.004802033\n",
      "\n",
      "What as your reaction when your heard about the scraping currency notes of INR 500 and 1000?\n",
      "What is your opinion about Narendra modis banning on 500 and 1000?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "opinion narendra modis ban \n",
      "reaction hear scrap currency note inr \n",
      "\n",
      "Feature Space------\n",
      "[  0.           0.           0.           5.72602208   9.44360924\n",
      "   8.17760983   0.54757338   1.0970105    0.83545248  77.58064796\n",
      " 126.539905   111.40163527   7.42617378  11.02646959   9.45966642\n",
      "   0.67892507   1.14458251   0.94176402 102.11101104 142.63602656\n",
      " 124.39314314]\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fn_idx = class_errors_df.sort_values('diff', ascending = False).head().index\n",
    "for idx in fn_idx:\n",
    "    print('Prob:', y_probs[idx])\n",
    "    print()\n",
    "    print(X_train.iloc[idx].question1)\n",
    "    print(X_train.iloc[idx].question2)\n",
    "    print()\n",
    "    print('Lemma--------')\n",
    "    print()\n",
    "    print(X_train_lemma[idx*2])\n",
    "    print(X_train_lemma[idx*2+1])\n",
    "    print()\n",
    "    print('Feature Space------')\n",
    "    print(X_transform[idx])\n",
    "    print('-------------------------------------------')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top false positive errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob: 0.98156744\n",
      "\n",
      "How 2000 rupee note stops black money?\n",
      "Do you think the 2000 rupee notes will increase black money?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "think  rupee note increase black money\n",
      " rupee note stop black money\n",
      "\n",
      "Feature Space------\n",
      "[  0.76923077   0.61538462   0.46153846   6.44314494  10.03362521\n",
      "   7.9075828    0.57044912   0.98469754   0.78002878  86.52876364\n",
      " 137.55217186 107.61267939   5.47033709  10.03362521   7.62770309\n",
      "   0.53674589   0.98469754   0.73902437  73.17100963 137.55217186\n",
      " 102.76879175]\n",
      "-------------------------------------------\n",
      "\n",
      "Prob: 0.9380506\n",
      "\n",
      "Why do people ask questions on Quora that can easily be answered by Google?\n",
      "Why do people use Quora to ask questions when Google or Wikipedia would be sufficient?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "people use quora ask question google wikipedia sufficient\n",
      "people ask question quora easily answer google\n",
      "\n",
      "Feature Space------\n",
      "[6.66666667e-01 1.33333333e-01 0.00000000e+00 3.54939540e+00\n",
      " 1.02873433e+01 6.97978315e+00 2.03129739e-01 1.19576709e+00\n",
      " 6.79430514e-01 4.94086112e+01 1.38667841e+02 9.37634244e+01\n",
      " 4.57339153e+00 1.00165953e+01 7.14255837e+00 3.35699168e-01\n",
      " 1.14565285e+00 7.17501372e-01 6.20160041e+01 1.29971439e+02\n",
      " 9.49136603e+01]\n",
      "-------------------------------------------\n",
      "\n",
      "Prob: 0.93492824\n",
      "\n",
      "Did Donald Trump win the election?\n",
      "How and Why did Donald Trump win the election?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "donald trump win election\n",
      "donald trump win election\n",
      "\n",
      "Feature Space------\n",
      "[  1.           1.           1.           6.07343053   9.06000318\n",
      "   7.7654374    0.51267833   0.93808649   0.71268391  82.2502204\n",
      " 120.30318822 103.19247649   6.07343053   9.06000318   7.7654374\n",
      "   0.51267833   0.93808649   0.71268391  82.2502204  120.30318822\n",
      " 103.19247649]\n",
      "-------------------------------------------\n",
      "\n",
      "Prob: 0.93492824\n",
      "\n",
      "What if Donald Trump winning this elections?\n",
      "Did Donald Trump win the election?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "donald trump win election\n",
      "donald trump win election\n",
      "\n",
      "Feature Space------\n",
      "[  1.           1.           1.           6.07343053   9.06000318\n",
      "   7.7654374    0.51267833   0.93808649   0.71268391  82.2502204\n",
      " 120.30318822 103.19247649   6.07343053   9.06000318   7.7654374\n",
      "   0.51267833   0.93808649   0.71268391  82.2502204  120.30318822\n",
      " 103.19247649]\n",
      "-------------------------------------------\n",
      "\n",
      "Prob: 0.9228907\n",
      "\n",
      "Who is marking my question on Quora as needing improvement and what are their reasons?\n",
      "What should I do if Quora marks my question as \"Needs Improvement\"?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "quora mark question need improvement\n",
      "mark question quora need improvement reason\n",
      "\n",
      "Feature Space------\n",
      "[  0.90909091   0.36363636   0.           4.2548659    9.9858694\n",
      "   7.24335932   0.35106545   1.15707429   0.76463828  58.09165457\n",
      " 132.39506995  97.0464529    5.27005875   9.9858694    7.79023608\n",
      "   0.47044656   1.15707429   0.83258831  72.17102902 132.39506995\n",
      " 104.50935466]\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fn_idx = class_errors_df.sort_values('diff').head().index\n",
    "for idx in fn_idx:\n",
    "    print('Prob:', y_probs[idx])\n",
    "    print()\n",
    "    print(X_train.iloc[idx].question1)\n",
    "    print(X_train.iloc[idx].question2)\n",
    "    print()\n",
    "    print('Lemma--------')\n",
    "    print()\n",
    "    print(X_train_lemma[idx*2])\n",
    "    print(X_train_lemma[idx*2+1])\n",
    "    print()\n",
    "    print('Feature Space------')\n",
    "    print(X_transform[idx])\n",
    "    print('-------------------------------------------')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Adding the same set of fetures for the non-lemmatized, but cleaned question can help with some of the examples. This will be the next model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
