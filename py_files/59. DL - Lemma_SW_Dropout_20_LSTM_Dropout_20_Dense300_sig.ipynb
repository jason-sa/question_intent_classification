{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MVP Siamese LSTM Net\n",
    "\n",
    "This is a baseline siamese LSTM net. The purpose is to build out the architecture, and see if the net can get as good as validation score as the classifiers.\n",
    "\n",
    "Ideas Implemented:\n",
    "* Dense layer with sigmoid activation\n",
    "* Dense layer with same dimension as LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:05:40.872824Z",
     "start_time": "2018-11-29T00:05:40.835246Z"
    }
   },
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Input, Add, concatenate, BatchNormalization, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# plotting\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:54:40.060662Z",
     "start_time": "2018-11-28T23:54:39.617510Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = utils.load('X_train')\n",
    "y_train = utils.load('y_train')\n",
    "model_name = 'lstm_LEMMA_SW_dropout_20_lstm_layer_DO_20'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Encode vocabulary\n",
    "\n",
    "1. Limit the vocab to 20,000 words.\n",
    "2. Clean questions only and do not lemmatize.\n",
    "3. Limit the question length to 100 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:54:48.602373Z",
     "start_time": "2018-11-28T23:54:40.063608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(606398, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = 20000\n",
    "max_q_len = 100\n",
    "\n",
    "X_train_stack = utils.apply_lemma(\n",
    "        utils.clean_questions(\n",
    "                utils.stack_questions(X_train), \n",
    "                excl_num=False), \n",
    "        incl_stop_words=False)\n",
    "\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(X_train_stack)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(X_train_stack)\n",
    "data = pad_sequences(sequences, maxlen=max_q_len)\n",
    "\n",
    "print(data.shape)\n",
    "data[:,0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Matrix\n",
    "\n",
    "1. Calculates the embedding matrix utilizing spaCy `en_core_web_lg` word vectors.\n",
    "  * https://spacy.io/models/en#en_core_web_lg\n",
    "  * GloVe vectors trained on Common Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:59:00.965247Z",
     "start_time": "2018-11-28T23:55:20.356138Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    embedding_matrix = utils.load('embedding_matrix_lemma_sw')\n",
    "except:\n",
    "    # create a weight matrix for words in training docs\n",
    "    embedding_matrix = np.zeros((vocabulary_size, 300))\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "    #     print(word, index, end='\\r')\n",
    "        if index > vocabulary_size - 1:\n",
    "            break\n",
    "        else:\n",
    "            embedding_vector = utils.nlp(word).vector\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[index] = embedding_vector\n",
    "    #     break\n",
    "\n",
    "    utils.save(embedding_matrix, 'embedding_matrix_lemma_sw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the batch to pass into the network\n",
    "\n",
    "Create arrays to split the stacked data into question 1 set and question 2 set for each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:25:57.617365Z",
     "start_time": "2018-11-29T00:25:57.429291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train major class: 0.63\n",
      "Val major class: 0.63\n"
     ]
    }
   ],
   "source": [
    "# cooncatenate the two questions\n",
    "odd_idx = [i for i in range(data.shape[0]) if i % 2 == 1]\n",
    "even_idx = [i for i in range(data.shape[0]) if i % 2 == 0]\n",
    "\n",
    "data_1 = data[odd_idx]\n",
    "data_2 = data[odd_idx]\n",
    "\n",
    "# split the data set into a validation set\n",
    "data_train, data_val, label_train, label_val = train_test_split(np.hstack([data_1, data_2]), \n",
    "                                                                y_train, \n",
    "                                                                stratify=y_train, \n",
    "                                                                test_size = 0.33,\n",
    "                                                                random_state=42)\n",
    "\n",
    "# split the concatenation back into 2 data sets for the siamese network\n",
    "data_1_train = data_train[:, :max_q_len]\n",
    "data_2_train = data_train[:, max_q_len:]\n",
    "data_1_val = data_val[:, :max_q_len]\n",
    "data_2_val = data_val[:, max_q_len:]\n",
    "\n",
    "print(f'Train major class: {len(label_train[label_train == 0]) / len(label_train):.2}')\n",
    "print(f'Val major class: {len(label_val[label_val == 0]) / len(label_val):.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build out legs of the siamese network\n",
    "\n",
    "The architecure is the following,\n",
    "\n",
    "0. Input - (100,) word tensor\n",
    "1. Embedding Layer - outputs (300,) **not trainable**\n",
    "2. LSTM - default outputs (300,)\n",
    "3. Concatenate the two nets outputs (600,)\n",
    "4. BatchNormalization\n",
    "5. Dropout - 20%\n",
    "6. Dense - outputs (100,), activation `tanh` -- somewhat random decision\n",
    "7. BatchNormalization\n",
    "8. Dropout - 20%\n",
    "9. Dense - outputs (1,), activation `sigmoid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:15:21.951772Z",
     "start_time": "2018-11-29T00:15:19.924658Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating word embedding layer\n",
    "embedding_layer = Embedding(vocabulary_size, 300, input_length=100, \n",
    "                                     weights=[embedding_matrix], trainable=False)\n",
    "\n",
    "# Creating LSTM Encoder\n",
    "# Bidirectional(LSTM(self.number_lstm_units, dropout=self.rate_drop_lstm, recurrent_dropout=self.rate_drop_lstm))\n",
    "lstm_layer = Bidirectional(LSTM(300, dropout=0.17, recurrent_dropout=0.17))\n",
    "\n",
    "# Creating LSTM Encoder layer for First Sentence\n",
    "sequence_1_input = Input(shape=(100,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "# Creating LSTM Encoder layer for Second Sentence\n",
    "sequence_2_input = Input(shape=(100,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "x2 = lstm_layer(embedded_sequences_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:16:12.624956Z",
     "start_time": "2018-11-29T00:16:12.372819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 100, 300)     6000000     input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 600)          1442400     embedding_3[0][0]                \n",
      "                                                                 embedding_3[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1200)         0           bidirectional_2[0][0]            \n",
      "                                                                 bidirectional_2[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 1200)         4800        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1200)         0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 300)          360300      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 300)          1200        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 300)          0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            301         dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,809,001\n",
      "Trainable params: 1,806,001\n",
      "Non-trainable params: 6,003,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Merging two LSTM encodes vectors from sentences to\n",
    "# pass it to dense layer applying dropout and batch normalisation\n",
    "\n",
    "merged = concatenate([x1, x2])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(.25)(merged)\n",
    "merged = Dense(300, activation='relu')(merged) # feed forward\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(0.25)(merged)\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "# SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "\n",
    "file_path = '../data/keras_models/' + model_name + '_{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(filepath=file_path, save_best_only=True)\n",
    "\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='../data/tensorboard')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                               min_delta=0, \n",
    "                               patience=3, \n",
    "                               verbose=1, \n",
    "                               mode='auto', \n",
    "                               restore_best_weights=True)\n",
    "\n",
    "# calc_auc = IntervalEvaluation(([data_1_val, data_2_val], label_val), interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-29T00:26:39.525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203143 samples, validate on 100056 samples\n",
      "Epoch 1/200\n",
      "203143/203143 [==============================] - 1650s 8ms/step - loss: 0.6033 - acc: 0.6809 - val_loss: 0.5732 - val_acc: 0.7079\n",
      "Epoch 2/200\n",
      "203143/203143 [==============================] - 1671s 8ms/step - loss: 0.5574 - acc: 0.7157 - val_loss: 0.5505 - val_acc: 0.7228\n",
      "Epoch 3/200\n",
      "203143/203143 [==============================] - 1563s 8ms/step - loss: 0.5392 - acc: 0.7297 - val_loss: 0.5373 - val_acc: 0.7362\n",
      "Epoch 4/200\n",
      "203143/203143 [==============================] - 1683s 8ms/step - loss: 0.5162 - acc: 0.7448 - val_loss: 0.5248 - val_acc: 0.7399\n",
      "Epoch 5/200\n",
      " 18688/203143 [=>............................] - ETA: 21:56 - loss: 0.4848 - acc: 0.7647"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b3dfebd2ae1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_1_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_2_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                   callbacks=[model_checkpoint, tensorboard, early_stopping])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit([data_1_train, data_2_train], label_train, \n",
    "          validation_data=([data_1_val, data_2_val], label_val),\n",
    "                  epochs=200, batch_size=128, shuffle=True,\n",
    "                  callbacks=[model_checkpoint, tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100056/100056 [==============================] - 100s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# model = load_model('../data/keras_models/lstm_LEMMA_SW_dropout_20_lstm_layer_DO_20_08-0.52.hdf5')\n",
    "\n",
    "y_prob = model.predict([data_1_val, data_2_val], batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_accuracy</th>\n",
       "      <th>std_accuracy</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>std_precision</th>\n",
       "      <th>avg_recall</th>\n",
       "      <th>std_recall</th>\n",
       "      <th>avg_f1</th>\n",
       "      <th>std_f1</th>\n",
       "      <th>avg_auc</th>\n",
       "      <th>std_auc</th>\n",
       "      <th>avg_log_loss</th>\n",
       "      <th>std_log_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rf_feat_eng_model_lemma_clean</th>\n",
       "      <td>0.783667</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>0.708853</td>\n",
       "      <td>0.003681</td>\n",
       "      <td>0.702725</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>0.705774</td>\n",
       "      <td>0.002658</td>\n",
       "      <td>0.868202</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.436197</td>\n",
       "      <td>0.000640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensemble_rf_xgb</th>\n",
       "      <td>0.779000</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.697794</td>\n",
       "      <td>0.004357</td>\n",
       "      <td>0.708157</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.702935</td>\n",
       "      <td>0.003148</td>\n",
       "      <td>0.863334</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.441784</td>\n",
       "      <td>0.001107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb_feat_eng_incl_nums</th>\n",
       "      <td>0.767110</td>\n",
       "      <td>0.001576</td>\n",
       "      <td>0.682213</td>\n",
       "      <td>0.002621</td>\n",
       "      <td>0.701238</td>\n",
       "      <td>0.002695</td>\n",
       "      <td>0.691590</td>\n",
       "      <td>0.001899</td>\n",
       "      <td>0.851957</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.450099</td>\n",
       "      <td>0.001675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_eng_model_lemma_clean</th>\n",
       "      <td>0.763927</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.676166</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>0.692113</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.684044</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.846923</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>0.456929</td>\n",
       "      <td>0.001410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_eng_model_lemma_fix</th>\n",
       "      <td>0.744356</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>0.664513</td>\n",
       "      <td>0.004333</td>\n",
       "      <td>0.621357</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.642201</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>0.822197</td>\n",
       "      <td>0.001710</td>\n",
       "      <td>0.488131</td>\n",
       "      <td>0.001342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_eng_model</th>\n",
       "      <td>0.743614</td>\n",
       "      <td>0.002021</td>\n",
       "      <td>0.664102</td>\n",
       "      <td>0.003502</td>\n",
       "      <td>0.618400</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>0.640434</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>0.821070</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.489465</td>\n",
       "      <td>0.001141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensemble_rf_xgb_cos_sim</th>\n",
       "      <td>0.738700</td>\n",
       "      <td>0.007359</td>\n",
       "      <td>0.661290</td>\n",
       "      <td>0.010948</td>\n",
       "      <td>0.612827</td>\n",
       "      <td>0.009669</td>\n",
       "      <td>0.636128</td>\n",
       "      <td>0.009994</td>\n",
       "      <td>0.819987</td>\n",
       "      <td>0.005193</td>\n",
       "      <td>0.493703</td>\n",
       "      <td>0.003901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_dropout_50</th>\n",
       "      <td>0.751849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.690400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.594510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.638877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.802315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.570912</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_LEMMA_dropout_20_lstm_layer_DO_20</th>\n",
       "      <td>0.752119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.708745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.557914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.801938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.561581</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_mvp</th>\n",
       "      <td>0.749760</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.685627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.595133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.637183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.801019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.643059</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cos_sim_tfidf_model</th>\n",
       "      <td>0.729511</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.661680</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.547188</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.599010</td>\n",
       "      <td>0.001703</td>\n",
       "      <td>0.800271</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>0.512085</td>\n",
       "      <td>0.001299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_LEMMA_SW_dropout_20_lstm_layer_DO_20</th>\n",
       "      <td>0.747411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.694125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.794318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.724173</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_dropout_50_lstm_layer_DO_50</th>\n",
       "      <td>0.736538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.578080</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.618360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.785850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.099757</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_dropout50_dense50_BatchNorm</th>\n",
       "      <td>0.728612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.682992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.573654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.772344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.373478</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cos_sim_model</th>\n",
       "      <td>0.710200</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.658748</td>\n",
       "      <td>0.002578</td>\n",
       "      <td>0.446336</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>0.532120</td>\n",
       "      <td>0.001306</td>\n",
       "      <td>0.746769</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.565250</td>\n",
       "      <td>0.000963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mvp (tf-idf, nmf(5), xgboost)</th>\n",
       "      <td>0.700345</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.661571</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.385736</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>0.487325</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.740593</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.568958</td>\n",
       "      <td>0.001288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mvp (+ lemma)</th>\n",
       "      <td>0.696787</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.649977</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.387424</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>0.485464</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>0.738037</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.572483</td>\n",
       "      <td>0.000815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           avg_accuracy  std_accuracy  \\\n",
       "rf_feat_eng_model_lemma_clean                  0.783667      0.002260   \n",
       "ensemble_rf_xgb                                0.779000      0.002740   \n",
       "xgb_feat_eng_incl_nums                         0.767110      0.001576   \n",
       "feat_eng_model_lemma_clean                     0.763927      0.002404   \n",
       "feat_eng_model_lemma_fix                       0.744356      0.002107   \n",
       "feat_eng_model                                 0.743614      0.002021   \n",
       "ensemble_rf_xgb_cos_sim                        0.738700      0.007359   \n",
       "lstm_dropout_50                                0.751849      0.000000   \n",
       "lstm_LEMMA_dropout_20_lstm_layer_DO_20         0.752119      0.000000   \n",
       "lstm_mvp                                       0.749760      0.000000   \n",
       "cos_sim_tfidf_model                            0.729511      0.001216   \n",
       "lstm_LEMMA_SW_dropout_20_lstm_layer_DO_20      0.747411      0.000000   \n",
       "lstm_dropout_50_lstm_layer_DO_50               0.736538      0.000000   \n",
       "lstm_dropout50_dense50_BatchNorm               0.728612      0.000000   \n",
       "cos_sim_model                                  0.710200      0.000830   \n",
       "mvp (tf-idf, nmf(5), xgboost)                  0.700345      0.000466   \n",
       "mvp (+ lemma)                                  0.696787      0.001055   \n",
       "\n",
       "                                           avg_precision  std_precision  \\\n",
       "rf_feat_eng_model_lemma_clean                   0.708853       0.003681   \n",
       "ensemble_rf_xgb                                 0.697794       0.004357   \n",
       "xgb_feat_eng_incl_nums                          0.682213       0.002621   \n",
       "feat_eng_model_lemma_clean                      0.676166       0.003904   \n",
       "feat_eng_model_lemma_fix                        0.664513       0.004333   \n",
       "feat_eng_model                                  0.664102       0.003502   \n",
       "ensemble_rf_xgb_cos_sim                         0.661290       0.010948   \n",
       "lstm_dropout_50                                 0.690400       0.000000   \n",
       "lstm_LEMMA_dropout_20_lstm_layer_DO_20          0.708745       0.000000   \n",
       "lstm_mvp                                        0.685627       0.000000   \n",
       "cos_sim_tfidf_model                             0.661680       0.002219   \n",
       "lstm_LEMMA_SW_dropout_20_lstm_layer_DO_20       0.694125       0.000000   \n",
       "lstm_dropout_50_lstm_layer_DO_50                0.664675       0.000000   \n",
       "lstm_dropout50_dense50_BatchNorm                0.682992       0.000000   \n",
       "cos_sim_model                                   0.658748       0.002578   \n",
       "mvp (tf-idf, nmf(5), xgboost)                   0.661571       0.000461   \n",
       "mvp (+ lemma)                                   0.649977       0.003057   \n",
       "\n",
       "                                           avg_recall  std_recall    avg_f1  \\\n",
       "rf_feat_eng_model_lemma_clean                0.702725    0.001666  0.705774   \n",
       "ensemble_rf_xgb                              0.708157    0.001912  0.702935   \n",
       "xgb_feat_eng_incl_nums                       0.701238    0.002695  0.691590   \n",
       "feat_eng_model_lemma_clean                   0.692113    0.001128  0.684044   \n",
       "feat_eng_model_lemma_fix                     0.621357    0.000901  0.642201   \n",
       "feat_eng_model                               0.618400    0.001553  0.640434   \n",
       "ensemble_rf_xgb_cos_sim                      0.612827    0.009669  0.636128   \n",
       "lstm_dropout_50                              0.594510    0.000000  0.638877   \n",
       "lstm_LEMMA_dropout_20_lstm_layer_DO_20       0.557914    0.000000  0.624349   \n",
       "lstm_mvp                                     0.595133    0.000000  0.637183   \n",
       "cos_sim_tfidf_model                          0.547188    0.001744  0.599010   \n",
       "lstm_LEMMA_SW_dropout_20_lstm_layer_DO_20    0.564762    0.000000  0.622797   \n",
       "lstm_dropout_50_lstm_layer_DO_50             0.578080    0.000000  0.618360   \n",
       "lstm_dropout50_dense50_BatchNorm             0.494492    0.000000  0.573654   \n",
       "cos_sim_model                                0.446336    0.002215  0.532120   \n",
       "mvp (tf-idf, nmf(5), xgboost)                0.385736    0.002493  0.487325   \n",
       "mvp (+ lemma)                                0.387424    0.003230  0.485464   \n",
       "\n",
       "                                             std_f1   avg_auc   std_auc  \\\n",
       "rf_feat_eng_model_lemma_clean              0.002658  0.868202  0.001148   \n",
       "ensemble_rf_xgb                            0.003148  0.863334  0.001438   \n",
       "xgb_feat_eng_incl_nums                     0.001899  0.851957  0.001192   \n",
       "feat_eng_model_lemma_clean                 0.002549  0.846923  0.001643   \n",
       "feat_eng_model_lemma_fix                   0.001609  0.822197  0.001710   \n",
       "feat_eng_model                             0.002281  0.821070  0.001428   \n",
       "ensemble_rf_xgb_cos_sim                    0.009994  0.819987  0.005193   \n",
       "lstm_dropout_50                            0.000000  0.802315  0.000000   \n",
       "lstm_LEMMA_dropout_20_lstm_layer_DO_20     0.000000  0.801938  0.000000   \n",
       "lstm_mvp                                   0.000000  0.801019  0.000000   \n",
       "cos_sim_tfidf_model                        0.001703  0.800271  0.001291   \n",
       "lstm_LEMMA_SW_dropout_20_lstm_layer_DO_20  0.000000  0.794318  0.000000   \n",
       "lstm_dropout_50_lstm_layer_DO_50           0.000000  0.785850  0.000000   \n",
       "lstm_dropout50_dense50_BatchNorm           0.000000  0.772344  0.000000   \n",
       "cos_sim_model                              0.001306  0.746769  0.001279   \n",
       "mvp (tf-idf, nmf(5), xgboost)              0.001983  0.740593  0.001647   \n",
       "mvp (+ lemma)                              0.002485  0.738037  0.001362   \n",
       "\n",
       "                                           avg_log_loss  std_log_loss  \n",
       "rf_feat_eng_model_lemma_clean                  0.436197      0.000640  \n",
       "ensemble_rf_xgb                                0.441784      0.001107  \n",
       "xgb_feat_eng_incl_nums                         0.450099      0.001675  \n",
       "feat_eng_model_lemma_clean                     0.456929      0.001410  \n",
       "feat_eng_model_lemma_fix                       0.488131      0.001342  \n",
       "feat_eng_model                                 0.489465      0.001141  \n",
       "ensemble_rf_xgb_cos_sim                        0.493703      0.003901  \n",
       "lstm_dropout_50                                8.570912      0.000000  \n",
       "lstm_LEMMA_dropout_20_lstm_layer_DO_20         8.561581      0.000000  \n",
       "lstm_mvp                                       8.643059      0.000000  \n",
       "cos_sim_tfidf_model                            0.512085      0.001299  \n",
       "lstm_LEMMA_SW_dropout_20_lstm_layer_DO_20      8.724173      0.000000  \n",
       "lstm_dropout_50_lstm_layer_DO_50               9.099757      0.000000  \n",
       "lstm_dropout50_dense50_BatchNorm               9.373478      0.000000  \n",
       "cos_sim_model                                  0.565250      0.000963  \n",
       "mvp (tf-idf, nmf(5), xgboost)                  0.568958      0.001288  \n",
       "mvp (+ lemma)                                  0.572483      0.000815  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = utils.load('results')\n",
    "\n",
    "results_df = results_df.drop(index=model_name, errors='ignore')\n",
    "results_df = results_df.append(utils.log_keras_scores(label_val, y_prob, model_name))\n",
    "results_df.sort_values('avg_auc', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save(results_df, 'results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "Similar to the best NN model yet. Let's build upon this model.\n",
    "\n",
    "* Dense layer with sigmoid activation\n",
    "* Dense layer with same dimension as LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
