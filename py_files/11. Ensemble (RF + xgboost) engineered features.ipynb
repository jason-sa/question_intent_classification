{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced featrue engineer model\n",
    "\n",
    "This model will add engineered features for the original question, in addition to the lemmatized question.\n",
    "\n",
    "Engineered two different types of features,\n",
    "\n",
    "1. n_gram similarity between each pair of questions\n",
    "2. min/max/avg distance between words in a single question. Currently using the following metrics,\n",
    "  * euclidean\n",
    "  * cosine\n",
    "  * city block or manhattan\n",
    "  \n",
    "**Pipeline**\n",
    "1. Stack questions\n",
    "2. Clean questions - now lower cases all words to better lemmatize proper nouns\n",
    "3. UNION\n",
    "    1. n_gram similarity\n",
    "    2. min/max/avg distance\n",
    "4. Lemmatize questions\n",
    "5. UNION\n",
    "    1. n_gram similarity\n",
    "    2. min/max/avg distances\n",
    "6. UNION together both sets of features\n",
    "7. Avearge Ensemble\n",
    "    1. Random Forrest\n",
    "    2. XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T04:57:04.646711Z",
     "start_time": "2018-11-28T04:56:47.834474Z"
    }
   },
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# modeling\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T04:57:05.108061Z",
     "start_time": "2018-11-28T04:57:04.649533Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = utils.load('X_train')\n",
    "y_train = utils.load('y_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T04:57:05.128186Z",
     "start_time": "2018-11-28T04:57:05.110672Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature engineering pipes\n",
    "single_question_pipe = Pipeline(\n",
    "    [\n",
    "        ('dist', FunctionTransformer(utils.add_min_max_avg_distance_features, validate=False)),\n",
    "        ('unstack', FunctionTransformer(utils.unstack_questions, validate=False))\n",
    "    ]\n",
    ")\n",
    "\n",
    "pair_question_pipe = Pipeline(\n",
    "    [\n",
    "        ('ngram_sim', FunctionTransformer(utils.calc_ngram_similarity, kw_args={'n_grams':[1, 2, 3]}, validate=False))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# clean text pipe\n",
    "clean_text_pipe = Pipeline(\n",
    "    [\n",
    "        ('stack', FunctionTransformer(utils.stack_questions, validate=False)),\n",
    "        ('clean', FunctionTransformer(utils.clean_questions, validate=False)),\n",
    "        ('feats', FeatureUnion(\n",
    "            [\n",
    "                ('pair', pair_question_pipe),\n",
    "                ('single', single_question_pipe)\n",
    "            ]\n",
    "        ))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# lemma pipe\n",
    "lemma_pipe = Pipeline(\n",
    "    [\n",
    "        ('stack', FunctionTransformer(utils.stack_questions, validate=False)),\n",
    "        ('clean', FunctionTransformer(utils.clean_questions, validate=False)),\n",
    "        ('lemma', FunctionTransformer(utils.apply_lemma, validate=False)),\n",
    "        ('feats', FeatureUnion(\n",
    "            [\n",
    "                ('pair', pair_question_pipe),\n",
    "                ('single', single_question_pipe)\n",
    "            ]\n",
    "        ))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# pre-process pipe\n",
    "pre_process_pipe = Pipeline(\n",
    "    [\n",
    "        ('feats', FeatureUnion(\n",
    "            [\n",
    "                ('clean_features', clean_text_pipe),\n",
    "                ('lemma_pipe', lemma_pipe)\n",
    "            ]\n",
    "        ))\n",
    "    ]\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)\n",
    "xgb = XGBClassifier(n_estimators=500, n_job=-1, random_state=42)\n",
    "vc = VotingClassifier([('rf', rf), ('xgb', xgb)], voting='soft', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T04:57:09.161680Z",
     "start_time": "2018-11-28T04:57:05.134427Z"
    }
   },
   "outputs": [],
   "source": [
    "X_transform = pre_process_pipe.transform(X_train)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3, random_state=42)\n",
    "cv = cross_validate(vc, \n",
    "               X_transform, \n",
    "               y_train, \n",
    "               cv=skf, \n",
    "               n_jobs=-1, \n",
    "               scoring=('accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'neg_log_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_accuracy</th>\n",
       "      <th>std_accuracy</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>std_precision</th>\n",
       "      <th>avg_recall</th>\n",
       "      <th>std_recall</th>\n",
       "      <th>avg_f1</th>\n",
       "      <th>std_f1</th>\n",
       "      <th>avg_auc</th>\n",
       "      <th>std_auc</th>\n",
       "      <th>avg_log_loss</th>\n",
       "      <th>std_log_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mvp (tf-idf, nmf(5), xgboost)</th>\n",
       "      <td>0.700345</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.661571</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.385736</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>0.487325</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.740593</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.568958</td>\n",
       "      <td>0.001288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mvp (+ lemma)</th>\n",
       "      <td>0.696787</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.649977</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.387424</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>0.485464</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>0.738037</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.572483</td>\n",
       "      <td>0.000815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cos_sim_model</th>\n",
       "      <td>0.710200</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.658748</td>\n",
       "      <td>0.002578</td>\n",
       "      <td>0.446336</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>0.532120</td>\n",
       "      <td>0.001306</td>\n",
       "      <td>0.746769</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.565250</td>\n",
       "      <td>0.000963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cos_sim_tfidf_model</th>\n",
       "      <td>0.728261</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>0.659662</td>\n",
       "      <td>0.002240</td>\n",
       "      <td>0.545419</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.597124</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>0.799173</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>0.513172</td>\n",
       "      <td>0.001191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_eng_model</th>\n",
       "      <td>0.743614</td>\n",
       "      <td>0.002021</td>\n",
       "      <td>0.664102</td>\n",
       "      <td>0.003502</td>\n",
       "      <td>0.618400</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>0.640434</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>0.821070</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.489465</td>\n",
       "      <td>0.001141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_eng_model_lemma_fix</th>\n",
       "      <td>0.744356</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>0.664513</td>\n",
       "      <td>0.004333</td>\n",
       "      <td>0.621357</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.642201</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>0.822197</td>\n",
       "      <td>0.001710</td>\n",
       "      <td>0.488131</td>\n",
       "      <td>0.001342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_eng_model_lemma_clean</th>\n",
       "      <td>0.763927</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.676166</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>0.692113</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.684044</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.846923</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>0.456929</td>\n",
       "      <td>0.001410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_feat_eng_model_lemma_clean</th>\n",
       "      <td>0.783667</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>0.708853</td>\n",
       "      <td>0.003681</td>\n",
       "      <td>0.702725</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>0.705774</td>\n",
       "      <td>0.002658</td>\n",
       "      <td>0.868202</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.436197</td>\n",
       "      <td>0.000640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensemble_rf_xgb</th>\n",
       "      <td>0.779000</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.697794</td>\n",
       "      <td>0.004357</td>\n",
       "      <td>0.708157</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.702935</td>\n",
       "      <td>0.003148</td>\n",
       "      <td>0.863334</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.441784</td>\n",
       "      <td>0.001107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               avg_accuracy  std_accuracy  avg_precision  \\\n",
       "mvp (tf-idf, nmf(5), xgboost)      0.700345      0.000466       0.661571   \n",
       "mvp (+ lemma)                      0.696787      0.001055       0.649977   \n",
       "cos_sim_model                      0.710200      0.000830       0.658748   \n",
       "cos_sim_tfidf_model                0.728261      0.001248       0.659662   \n",
       "feat_eng_model                     0.743614      0.002021       0.664102   \n",
       "feat_eng_model_lemma_fix           0.744356      0.002107       0.664513   \n",
       "feat_eng_model_lemma_clean         0.763927      0.002404       0.676166   \n",
       "rf_feat_eng_model_lemma_clean      0.783667      0.002260       0.708853   \n",
       "ensemble_rf_xgb                    0.779000      0.002740       0.697794   \n",
       "\n",
       "                               std_precision  avg_recall  std_recall  \\\n",
       "mvp (tf-idf, nmf(5), xgboost)       0.000461    0.385736    0.002493   \n",
       "mvp (+ lemma)                       0.003057    0.387424    0.003230   \n",
       "cos_sim_model                       0.002578    0.446336    0.002215   \n",
       "cos_sim_tfidf_model                 0.002240    0.545419    0.001370   \n",
       "feat_eng_model                      0.003502    0.618400    0.001553   \n",
       "feat_eng_model_lemma_fix            0.004333    0.621357    0.000901   \n",
       "feat_eng_model_lemma_clean          0.003904    0.692113    0.001128   \n",
       "rf_feat_eng_model_lemma_clean       0.003681    0.702725    0.001666   \n",
       "ensemble_rf_xgb                     0.004357    0.708157    0.001912   \n",
       "\n",
       "                                 avg_f1    std_f1   avg_auc   std_auc  \\\n",
       "mvp (tf-idf, nmf(5), xgboost)  0.487325  0.001983  0.740593  0.001647   \n",
       "mvp (+ lemma)                  0.485464  0.002485  0.738037  0.001362   \n",
       "cos_sim_model                  0.532120  0.001306  0.746769  0.001279   \n",
       "cos_sim_tfidf_model            0.597124  0.001666  0.799173  0.001407   \n",
       "feat_eng_model                 0.640434  0.002281  0.821070  0.001428   \n",
       "feat_eng_model_lemma_fix       0.642201  0.001609  0.822197  0.001710   \n",
       "feat_eng_model_lemma_clean     0.684044  0.002549  0.846923  0.001643   \n",
       "rf_feat_eng_model_lemma_clean  0.705774  0.002658  0.868202  0.001148   \n",
       "ensemble_rf_xgb                0.702935  0.003148  0.863334  0.001438   \n",
       "\n",
       "                               avg_log_loss  std_log_loss  \n",
       "mvp (tf-idf, nmf(5), xgboost)      0.568958      0.001288  \n",
       "mvp (+ lemma)                      0.572483      0.000815  \n",
       "cos_sim_model                      0.565250      0.000963  \n",
       "cos_sim_tfidf_model                0.513172      0.001191  \n",
       "feat_eng_model                     0.489465      0.001141  \n",
       "feat_eng_model_lemma_fix           0.488131      0.001342  \n",
       "feat_eng_model_lemma_clean         0.456929      0.001410  \n",
       "rf_feat_eng_model_lemma_clean      0.436197      0.000640  \n",
       "ensemble_rf_xgb                    0.441784      0.001107  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = utils.load('results')\n",
    "\n",
    "results_df = results_df.drop(index='ensemble_rf_xgb', errors='ignore')\n",
    "results_df = results_df.append(utils.log_scores(cv, 'ensemble_rf_xgb'))\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save(results_df, 'results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Wow! The feature engineering shows a significant jump in AUC from 0.8 to 0.82."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_we...ate=42, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=True, subsample=1))],\n",
       "         flatten_transform=None, n_jobs=None, voting='soft', weights=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc = VotingClassifier([('rf', rf), ('xgb', xgb)], voting='soft')\n",
    "vc.fit(X_transform, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gt</th>\n",
       "      <th>prob</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.271811</td>\n",
       "      <td>-0.271811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.331749</td>\n",
       "      <td>-0.331749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.158016</td>\n",
       "      <td>-0.158016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>-0.000237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.034689</td>\n",
       "      <td>-0.034689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gt      prob      diff\n",
       "0   0  0.271811 -0.271811\n",
       "1   0  0.331749 -0.331749\n",
       "2   0  0.158016 -0.158016\n",
       "3   0  0.000237 -0.000237\n",
       "4   0  0.034689 -0.034689"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_probs = vc.predict_proba(X_transform)[:, 1]\n",
    "class_errors_df = utils.ground_truth_analysis(y_train, y_probs)\n",
    "class_errors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_pipe = Pipeline(\n",
    "    [\n",
    "        ('stack', FunctionTransformer(utils.stack_questions, validate=False)),\n",
    "        ('clean', FunctionTransformer(utils.clean_questions, validate=False)),\n",
    "        ('lemma', FunctionTransformer(utils.apply_lemma, validate=False)),\n",
    "    ]\n",
    ")\n",
    "X_train_lemma = lemma_pipe.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top false negative errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob: 0.06066200464556905\n",
      "\n",
      "Is unifunds.com legit?\n",
      "Is unifunds legit?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "unifund legit\n",
      "unifundscom legit\n",
      "\n",
      "Feature Space------\n",
      "[ 0.66666667  0.33333333  0.33333333  6.57793499  6.57793499  6.57793499\n",
      "  0.732846    0.732846    0.732846   88.33192412 88.33192412 88.33192412\n",
      "  6.57793499  6.57793499  6.57793499  0.732846    0.732846    0.732846\n",
      " 88.33192412 88.33192412 88.33192412  0.5         0.5         0.5\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.        ]\n",
      "-------------------------------------------\n",
      "\n",
      "Prob: 0.14889085861018544\n",
      "\n",
      "What is the remainder when 2^100 is divided by 101?\n",
      "What is the remainder when [math]2^{100}[/math] is divided by 101?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "remainder  divide\n",
      "remainder  divide\n",
      "\n",
      "Feature Space------\n",
      "[  1.           1.           1.           0.           7.61300637\n",
      "   5.48716164   0.           0.83449638   0.57976533   0.\n",
      " 105.72734378  74.80031913   0.           7.61300637   5.48716164\n",
      "   0.           0.83449638   0.57976533   0.         105.72734378\n",
      "  74.80031913   1.           1.           1.           6.180452\n",
      "   6.180452     6.180452     0.59770131   0.59770131   0.59770131\n",
      "  83.15616088  83.15616088  83.15616088   6.180452     6.180452\n",
      "   6.180452     0.59770131   0.59770131   0.59770131  83.15616088\n",
      "  83.15616088  83.15616088]\n",
      "-------------------------------------------\n",
      "\n",
      "Prob: 0.21339836283567845\n",
      "\n",
      "How is 0! = 1?\n",
      "How is 0! =1?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Space------\n",
      "[ 0.66666667  0.33333333  0.          5.09450825  5.09450825  5.09450825\n",
      "  0.50282511  0.50282511  0.50282511 69.31513759 69.31513759 69.31513759\n",
      "  5.09450825  5.09450825  5.09450825  0.50282511  0.50282511  0.50282511\n",
      " 69.31513759 69.31513759 69.31513759  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.        ]\n",
      "-------------------------------------------\n",
      "\n",
      "Prob: 0.2522045292925394\n",
      "\n",
      "What is the value of |1/0|?\n",
      "What is the value of 1/0?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "value\n",
      "value\n",
      "\n",
      "Feature Space------\n",
      "[ 1.          1.          1.          3.67209771  6.39188453  5.27503753\n",
      "  0.28606134  0.61919654  0.49754824 49.32912163 89.91105918 71.80362861\n",
      "  3.67209771  6.39188453  5.27503753  0.28606134  0.61919654  0.49754824\n",
      " 49.32912163 89.91105918 71.80362861  1.          1.          1.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.        ]\n",
      "-------------------------------------------\n",
      "\n",
      "Prob: 0.272205407556483\n",
      "\n",
      "What is the revenue model of saavn.com?\n",
      "What is the revenue model of Saavn?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "revenue model saavncom\n",
      "revenue model saavn\n",
      "\n",
      "Feature Space------\n",
      "[  0.85714286   0.71428571   0.57142857   3.67209771   8.75161105\n",
      "   6.25420822   0.28606134   0.81867508   0.60940849  49.32912163\n",
      " 117.95052162  84.52491877   3.67209771   8.75161105   6.25420822\n",
      "   0.28606134   0.81867508   0.60940849  49.32912163 117.95052162\n",
      "  84.52491877   0.66666667   0.33333333   0.           8.75161105\n",
      "   8.75161105   8.75161105   0.81867508   0.81867508   0.81867508\n",
      " 117.95052162 117.95052162 117.95052162   8.75161105   8.75161105\n",
      "   8.75161105   0.81867508   0.81867508   0.81867508 117.95052162\n",
      " 117.95052162 117.95052162]\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fn_idx = class_errors_df.sort_values('diff', ascending = False).head().index\n",
    "for idx in fn_idx:\n",
    "    print('Prob:', y_probs[idx])\n",
    "    print()\n",
    "    print(X_train.iloc[idx].question1)\n",
    "    print(X_train.iloc[idx].question2)\n",
    "    print()\n",
    "    print('Lemma--------')\n",
    "    print()\n",
    "    print(X_train_lemma[idx*2])\n",
    "    print(X_train_lemma[idx*2+1])\n",
    "    print()\n",
    "    print('Feature Space------')\n",
    "    print(X_transform[idx])\n",
    "    print('-------------------------------------------')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top false positive errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob: 0.8096086367161561\n",
      "\n",
      "What was the significance of the battle of Somme, and how did this battle compare and contrast to the Battle of Taiyuan?\n",
      "What was the significance of the battle of Somme, and how did this battle compare and contrast to the Battle of Eslands River?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "significance battle somme battle compare contrast battle esland river\n",
      "significance battle somme battle compare contrast battle taiyuan\n",
      "\n",
      "Feature Space------\n",
      "[  0.90322581   0.87804878   0.8372093    0.           8.94758213\n",
      "   5.63428436   0.           1.14395334   0.58597778   0.\n",
      " 119.74790659  76.10832393   0.           9.47086476   5.85348526\n",
      "   0.           1.14395334   0.60099757   0.         123.61142434\n",
      "  79.3040757    0.76923077   0.70588235   0.58823529   0.\n",
      "   8.94758213   6.58424623   0.           1.07835326   0.69907214\n",
      "   0.         119.74790659  88.10770493   0.           9.47086476\n",
      "   7.08213208   0.           1.07835326   0.72237867   0.\n",
      " 123.61142434  95.47296559]\n",
      "-------------------------------------------\n",
      "\n",
      "Prob: 0.694531082023893\n",
      "\n",
      "What is the KVPY SA expected cut off for 2016?\n",
      "What is the expected cut off for KVPY SA 2014?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "expect cut kvpy sa\n",
      "kvpy sa expect cut\n",
      "\n",
      "Feature Space------\n",
      "[  1.           0.66666667   0.33333333   4.40340196   9.64937054\n",
      "   6.39549847   0.39708544   0.94398671   0.65261261  60.934365\n",
      " 133.81101881  87.15810009   4.40340196   9.64937054   6.39549847\n",
      "   0.39708544   0.94398671   0.65261261  60.934365   133.81101881\n",
      "  87.15810009   1.           0.5          0.           6.02678065\n",
      "   9.64937054   8.20440901   0.59935045   0.94398671   0.81765884\n",
      "  83.28883764 133.81101881 113.00172643   6.02678065   9.64937054\n",
      "   8.20440901   0.59935045   0.94398671   0.81765884  83.28883764\n",
      " 133.81101881 113.00172643]\n",
      "-------------------------------------------\n",
      "\n",
      "Prob: 0.6930098326410569\n",
      "\n",
      "What is the best way to invest $1,000?\n",
      "How should I invest $3,000?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "invest\n",
      "good way invest\n",
      "\n",
      "Feature Space------\n",
      "[  0.18181818   0.18181818   0.18181818   3.32768196   6.91181574\n",
      "   5.05989927   0.22358162   0.77860875   0.48962364  44.39012259\n",
      "  96.74042761  69.17308505   4.4516778    7.68992106   6.03018017\n",
      "   0.36869252   0.74090815   0.54074633  59.87225566 102.96853832\n",
      "  81.7893619    0.5          0.5          0.5          3.77246627\n",
      "   6.56454039   5.47342111   0.27768882   0.64414244   0.50415504\n",
      "  52.96125268  88.30038751  74.88121715   0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.        ]\n",
      "-------------------------------------------\n",
      "\n",
      "Prob: 0.6887114702837808\n",
      "\n",
      "What are your New Year's resolutions for 2017?\n",
      "What's your New Year's Resolution for 2015?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "s new year resolution\n",
      "new year resolution\n",
      "\n",
      "Feature Space------\n",
      "[  0.71428571   0.42857143   0.28571429   5.24309317   8.37244655\n",
      "   6.48900516   0.46673024   0.81789488   0.64424472  71.42885172\n",
      " 115.16762681  89.3986523    5.24309317   8.49602851   6.63755553\n",
      "   0.46673024   0.8549345    0.67079366  71.42885172 118.03976494\n",
      "  91.40606893   0.85714286   0.85714286   0.85714286   5.89369346\n",
      "   8.3148705    7.32384159   0.56213046   0.8018933    0.70287803\n",
      "  81.40739787 114.23596283 100.72437346   5.89369346   8.32736541\n",
      "   7.20656205   0.56213046   0.8549345    0.72872899  81.40739787\n",
      " 114.23596283  98.6442687 ]\n",
      "-------------------------------------------\n",
      "\n",
      "Prob: 0.6799076334294818\n",
      "\n",
      "What is your New Year's resolutions for 2017?\n",
      "What are some of your New Year's resolutions for 2015?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "new year resolution\n",
      "new year resolution\n",
      "\n",
      "Feature Space------\n",
      "[  0.75         0.625        0.625        4.59021853   8.37244655\n",
      "   6.40959873   0.41826118   0.88202725   0.64428342  62.61617418\n",
      " 115.16762681  88.43306174   4.04779856   8.37244655   6.08703228\n",
      "   0.31602096   0.81789488   0.59743259  55.42881328 115.16762681\n",
      "  83.75670545   1.           1.           1.           5.89369346\n",
      "   8.3148705    7.32384159   0.56213046   0.8018933    0.70287803\n",
      "  81.40739787 114.23596283 100.72437346   5.89369346   8.3148705\n",
      "   7.32384159   0.56213046   0.8018933    0.70287803  81.40739787\n",
      " 114.23596283 100.72437346]\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fn_idx = class_errors_df.sort_values('diff').head().index\n",
    "for idx in fn_idx:\n",
    "    print('Prob:', y_probs[idx])\n",
    "    print()\n",
    "    print(X_train.iloc[idx].question1)\n",
    "    print(X_train.iloc[idx].question2)\n",
    "    print()\n",
    "    print('Lemma--------')\n",
    "    print()\n",
    "    print(X_train_lemma[idx*2])\n",
    "    print(X_train_lemma[idx*2+1])\n",
    "    print()\n",
    "    print('Feature Space------')\n",
    "    print(X_transform[idx])\n",
    "    print('-------------------------------------------')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. The false negative errors appear to be related to numbers. \n",
    "  * Either stop removing numebers in the clean text proces, or create another set of features.\n",
    "2. The false positives seem very tricky. Found the Facebook InferSent model which embeds sentences to 4096 dimension.\n",
    "  * Could see if the distances between some of the false positive examples is different in this space.\n",
    "3. Could look at alignments of the two questions. (I think nltk)\n",
    "4. Add different word embeddings to get a different nuance, or even use the vector only data set from Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
