{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MVP Siamese LSTM Net\n",
    "\n",
    "This is a baseline siamese LSTM net. The purpose is to build out the architecture, and see if the net can get as good as validation score as the classifiers.\n",
    "\n",
    "Ideas Implemented:\n",
    "* Set Dropout rate to 20%\n",
    "* Set Dropout rate in LSTM to 20%\n",
    "* Add Lemmatization pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:05:40.872824Z",
     "start_time": "2018-11-29T00:05:40.835246Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# data manipulation\n",
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Input, Add, concatenate, BatchNormalization\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# plotting\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:54:40.060662Z",
     "start_time": "2018-11-28T23:54:39.617510Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = utils.load('X_train')\n",
    "y_train = utils.load('y_train')\n",
    "model_name = 'lstm_LEMMA_dropout_20_lstm_layer_DO_20'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Encode vocabulary\n",
    "\n",
    "1. Limit the vocab to 20,000 words.\n",
    "2. Clean questions only and do not lemmatize.\n",
    "3. Limit the question length to 100 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:54:48.602373Z",
     "start_time": "2018-11-28T23:54:40.063608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(606398, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7779"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = 20000\n",
    "max_q_len = 100\n",
    "\n",
    "X_train_stack = utils.apply_lemma(\n",
    "        utils.clean_questions(\n",
    "                utils.stack_questions(X_train), \n",
    "                excl_num=False), \n",
    "        incl_stop_words=True)\n",
    "\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(X_train_stack)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(X_train_stack)\n",
    "data = pad_sequences(sequences, maxlen=max_q_len)\n",
    "\n",
    "print(data.shape)\n",
    "data[:,0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Matrix\n",
    "\n",
    "1. Calculates the embedding matrix utilizing spaCy `en_core_web_lg` word vectors.\n",
    "  * https://spacy.io/models/en#en_core_web_lg\n",
    "  * GloVe vectors trained on Common Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:59:00.965247Z",
     "start_time": "2018-11-28T23:55:20.356138Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    embedding_matrix = utils.load('embedding_matrix_lemma')\n",
    "except:\n",
    "    # create a weight matrix for words in training docs\n",
    "    embedding_matrix = np.zeros((vocabulary_size, 300))\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "    #     print(word, index, end='\\r')\n",
    "        if index > vocabulary_size - 1:\n",
    "            break\n",
    "        else:\n",
    "            embedding_vector = utils.nlp(word).vector\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[index] = embedding_vector\n",
    "    #     break\n",
    "\n",
    "    utils.save(embedding_matrix, 'embedding_matrix_lemma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the batch to pass into the network\n",
    "\n",
    "Create arrays to split the stacked data into question 1 set and question 2 set for each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:25:57.617365Z",
     "start_time": "2018-11-29T00:25:57.429291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train major class: 0.63\n",
      "Val major class: 0.63\n"
     ]
    }
   ],
   "source": [
    "# cooncatenate the two questions\n",
    "odd_idx = [i for i in range(data.shape[0]) if i % 2 == 1]\n",
    "even_idx = [i for i in range(data.shape[0]) if i % 2 == 0]\n",
    "\n",
    "data_1 = data[odd_idx]\n",
    "data_2 = data[odd_idx]\n",
    "\n",
    "# split the data set into a validation set\n",
    "data_train, data_val, label_train, label_val = train_test_split(np.hstack([data_1, data_2]), \n",
    "                                                                y_train, \n",
    "                                                                stratify=y_train, \n",
    "                                                                test_size = 0.33,\n",
    "                                                                random_state=42)\n",
    "\n",
    "# split the concatenation back into 2 data sets for the siamese network\n",
    "data_1_train = data_train[:, :max_q_len]\n",
    "data_2_train = data_train[:, max_q_len:]\n",
    "data_1_val = data_val[:, :max_q_len]\n",
    "data_2_val = data_val[:, max_q_len:]\n",
    "\n",
    "print(f'Train major class: {len(label_train[label_train == 0]) / len(label_train):.2}')\n",
    "print(f'Val major class: {len(label_val[label_val == 0]) / len(label_val):.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build out legs of the siamese network\n",
    "\n",
    "The architecure is the following,\n",
    "\n",
    "0. Input - (100,) word tensor\n",
    "1. Embedding Layer - outputs (300,) **not trainable**\n",
    "2. LSTM - default outputs (300,)\n",
    "3. Concatenate the two nets outputs (600,)\n",
    "4. BatchNormalization\n",
    "5. Dropout - 20%\n",
    "6. Dense - outputs (100,), activation `tanh` -- somewhat random decision\n",
    "7. BatchNormalization\n",
    "8. Dropout - 20%\n",
    "9. Dense - outputs (1,), activation `sigmoid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:15:21.951772Z",
     "start_time": "2018-11-29T00:15:19.924658Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating word embedding layer\n",
    "embedding_layer = Embedding(vocabulary_size, 300, input_length=100, \n",
    "                                     weights=[embedding_matrix], trainable=False)\n",
    "\n",
    "# Creating LSTM Encoder\n",
    "# Bidirectional(LSTM(self.number_lstm_units, dropout=self.rate_drop_lstm, recurrent_dropout=self.rate_drop_lstm))\n",
    "lstm_layer = LSTM(300, dropout=0.2, recurrent_dropout=0.2)\n",
    "\n",
    "# Creating LSTM Encoder layer for First Sentence\n",
    "sequence_1_input = Input(shape=(100,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "# Creating LSTM Encoder layer for Second Sentence\n",
    "sequence_2_input = Input(shape=(100,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "x2 = lstm_layer(embedded_sequences_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:16:12.624956Z",
     "start_time": "2018-11-29T00:16:12.372819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 300)     6000000     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 300)          721200      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 600)          0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 600)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          60100       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            101         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,781,401\n",
      "Trainable params: 781,401\n",
      "Non-trainable params: 6,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Merging two LSTM encodes vectors from sentences to\n",
    "# pass it to dense layer applying dropout and batch normalisation\n",
    "\n",
    "merged = concatenate([x1, x2])\n",
    "# merged = BatchNormalization()(merged)\n",
    "merged = Dropout(.2)(merged)\n",
    "merged = Dense(100)(merged) # feed forward\n",
    "# merged = BatchNormalization()(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "# SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "\n",
    "file_path = '../data/keras_models/' + model_name + '_{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(filepath=file_path, save_best_only=True)\n",
    "\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='../data/tensorboard')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                               min_delta=0, \n",
    "                               patience=3, \n",
    "                               verbose=1, \n",
    "                               mode='auto', \n",
    "                               restore_best_weights=True)\n",
    "\n",
    "# calc_auc = IntervalEvaluation(([data_1_val, data_2_val], label_val), interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-29T00:26:39.525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203143 samples, validate on 100056 samples\n",
      "Epoch 1/200\n",
      "203143/203143 [==============================] - 689s 3ms/step - loss: 0.5767 - acc: 0.6992 - val_loss: 0.5510 - val_acc: 0.7204\n",
      "Epoch 2/200\n",
      "203143/203143 [==============================] - 657s 3ms/step - loss: 0.5335 - acc: 0.7335 - val_loss: 0.5315 - val_acc: 0.7364\n",
      "Epoch 3/200\n",
      "203143/203143 [==============================] - 805s 4ms/step - loss: 0.5075 - acc: 0.7494 - val_loss: 0.5152 - val_acc: 0.7469\n",
      "Epoch 4/200\n",
      "203143/203143 [==============================] - 803s 4ms/step - loss: 0.4818 - acc: 0.7668 - val_loss: 0.5258 - val_acc: 0.7495\n",
      "Epoch 5/200\n",
      "203143/203143 [==============================] - 617s 3ms/step - loss: 0.4552 - acc: 0.7827 - val_loss: 0.5134 - val_acc: 0.7521\n",
      "Epoch 6/200\n",
      "203143/203143 [==============================] - 657s 3ms/step - loss: 0.4368 - acc: 0.7942 - val_loss: 0.5399 - val_acc: 0.7485\n",
      "Epoch 7/200\n",
      "203143/203143 [==============================] - 803s 4ms/step - loss: 0.4188 - acc: 0.8037 - val_loss: 0.5388 - val_acc: 0.7442\n",
      "Epoch 8/200\n",
      "203143/203143 [==============================] - 895s 4ms/step - loss: 0.4015 - acc: 0.8132 - val_loss: 0.5696 - val_acc: 0.7427\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00008: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f644eac90f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([data_1_train, data_2_train], label_train, \n",
    "          validation_data=([data_1_val, data_2_val], label_val),\n",
    "                  epochs=200, batch_size=128, shuffle=True,\n",
    "                  callbacks=[model_checkpoint, tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100056/100056 [==============================] - 131s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# model = load_model('../data/keras_models/mvp_batch_norm08-0.54.hdf5')\n",
    "\n",
    "y_prob = model.predict([data_1_val, data_2_val], batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_accuracy</th>\n",
       "      <th>std_accuracy</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>std_precision</th>\n",
       "      <th>avg_recall</th>\n",
       "      <th>std_recall</th>\n",
       "      <th>avg_f1</th>\n",
       "      <th>std_f1</th>\n",
       "      <th>avg_auc</th>\n",
       "      <th>std_auc</th>\n",
       "      <th>avg_log_loss</th>\n",
       "      <th>std_log_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rf_feat_eng_model_lemma_clean</th>\n",
       "      <td>0.783667</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>0.708853</td>\n",
       "      <td>0.003681</td>\n",
       "      <td>0.702725</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>0.705774</td>\n",
       "      <td>0.002658</td>\n",
       "      <td>0.868202</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.436197</td>\n",
       "      <td>0.000640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensemble_rf_xgb</th>\n",
       "      <td>0.779000</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.697794</td>\n",
       "      <td>0.004357</td>\n",
       "      <td>0.708157</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.702935</td>\n",
       "      <td>0.003148</td>\n",
       "      <td>0.863334</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.441784</td>\n",
       "      <td>0.001107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb_feat_eng_incl_nums</th>\n",
       "      <td>0.767110</td>\n",
       "      <td>0.001576</td>\n",
       "      <td>0.682213</td>\n",
       "      <td>0.002621</td>\n",
       "      <td>0.701238</td>\n",
       "      <td>0.002695</td>\n",
       "      <td>0.691590</td>\n",
       "      <td>0.001899</td>\n",
       "      <td>0.851957</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.450099</td>\n",
       "      <td>0.001675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_eng_model_lemma_clean</th>\n",
       "      <td>0.763927</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.676166</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>0.692113</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.684044</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.846923</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>0.456929</td>\n",
       "      <td>0.001410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_eng_model_lemma_fix</th>\n",
       "      <td>0.744356</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>0.664513</td>\n",
       "      <td>0.004333</td>\n",
       "      <td>0.621357</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.642201</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>0.822197</td>\n",
       "      <td>0.001710</td>\n",
       "      <td>0.488131</td>\n",
       "      <td>0.001342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_eng_model</th>\n",
       "      <td>0.743614</td>\n",
       "      <td>0.002021</td>\n",
       "      <td>0.664102</td>\n",
       "      <td>0.003502</td>\n",
       "      <td>0.618400</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>0.640434</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>0.821070</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.489465</td>\n",
       "      <td>0.001141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensemble_rf_xgb_cos_sim</th>\n",
       "      <td>0.738700</td>\n",
       "      <td>0.007359</td>\n",
       "      <td>0.661290</td>\n",
       "      <td>0.010948</td>\n",
       "      <td>0.612827</td>\n",
       "      <td>0.009669</td>\n",
       "      <td>0.636128</td>\n",
       "      <td>0.009994</td>\n",
       "      <td>0.819987</td>\n",
       "      <td>0.005193</td>\n",
       "      <td>0.493703</td>\n",
       "      <td>0.003901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_dropout_50</th>\n",
       "      <td>0.751849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.690400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.594510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.638877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.802315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.570912</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_LEMMA_dropout_20_lstm_layer_DO_20</th>\n",
       "      <td>0.752119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.708745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.557914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.801938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.561581</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_mvp</th>\n",
       "      <td>0.749760</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.685627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.595133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.637183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.801019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.643059</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cos_sim_tfidf_model</th>\n",
       "      <td>0.729511</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.661680</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.547188</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.599010</td>\n",
       "      <td>0.001703</td>\n",
       "      <td>0.800271</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>0.512085</td>\n",
       "      <td>0.001299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_dropout_50_lstm_layer_DO_50</th>\n",
       "      <td>0.736538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.578080</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.618360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.785850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.099757</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_dropout50_dense50_BatchNorm</th>\n",
       "      <td>0.728612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.682992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.573654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.772344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.373478</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cos_sim_model</th>\n",
       "      <td>0.710200</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.658748</td>\n",
       "      <td>0.002578</td>\n",
       "      <td>0.446336</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>0.532120</td>\n",
       "      <td>0.001306</td>\n",
       "      <td>0.746769</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.565250</td>\n",
       "      <td>0.000963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mvp (tf-idf, nmf(5), xgboost)</th>\n",
       "      <td>0.700345</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.661571</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.385736</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>0.487325</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.740593</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.568958</td>\n",
       "      <td>0.001288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mvp (+ lemma)</th>\n",
       "      <td>0.696787</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.649977</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.387424</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>0.485464</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>0.738037</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.572483</td>\n",
       "      <td>0.000815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        avg_accuracy  std_accuracy  \\\n",
       "rf_feat_eng_model_lemma_clean               0.783667      0.002260   \n",
       "ensemble_rf_xgb                             0.779000      0.002740   \n",
       "xgb_feat_eng_incl_nums                      0.767110      0.001576   \n",
       "feat_eng_model_lemma_clean                  0.763927      0.002404   \n",
       "feat_eng_model_lemma_fix                    0.744356      0.002107   \n",
       "feat_eng_model                              0.743614      0.002021   \n",
       "ensemble_rf_xgb_cos_sim                     0.738700      0.007359   \n",
       "lstm_dropout_50                             0.751849      0.000000   \n",
       "lstm_LEMMA_dropout_20_lstm_layer_DO_20      0.752119      0.000000   \n",
       "lstm_mvp                                    0.749760      0.000000   \n",
       "cos_sim_tfidf_model                         0.729511      0.001216   \n",
       "lstm_dropout_50_lstm_layer_DO_50            0.736538      0.000000   \n",
       "lstm_dropout50_dense50_BatchNorm            0.728612      0.000000   \n",
       "cos_sim_model                               0.710200      0.000830   \n",
       "mvp (tf-idf, nmf(5), xgboost)               0.700345      0.000466   \n",
       "mvp (+ lemma)                               0.696787      0.001055   \n",
       "\n",
       "                                        avg_precision  std_precision  \\\n",
       "rf_feat_eng_model_lemma_clean                0.708853       0.003681   \n",
       "ensemble_rf_xgb                              0.697794       0.004357   \n",
       "xgb_feat_eng_incl_nums                       0.682213       0.002621   \n",
       "feat_eng_model_lemma_clean                   0.676166       0.003904   \n",
       "feat_eng_model_lemma_fix                     0.664513       0.004333   \n",
       "feat_eng_model                               0.664102       0.003502   \n",
       "ensemble_rf_xgb_cos_sim                      0.661290       0.010948   \n",
       "lstm_dropout_50                              0.690400       0.000000   \n",
       "lstm_LEMMA_dropout_20_lstm_layer_DO_20       0.708745       0.000000   \n",
       "lstm_mvp                                     0.685627       0.000000   \n",
       "cos_sim_tfidf_model                          0.661680       0.002219   \n",
       "lstm_dropout_50_lstm_layer_DO_50             0.664675       0.000000   \n",
       "lstm_dropout50_dense50_BatchNorm             0.682992       0.000000   \n",
       "cos_sim_model                                0.658748       0.002578   \n",
       "mvp (tf-idf, nmf(5), xgboost)                0.661571       0.000461   \n",
       "mvp (+ lemma)                                0.649977       0.003057   \n",
       "\n",
       "                                        avg_recall  std_recall    avg_f1  \\\n",
       "rf_feat_eng_model_lemma_clean             0.702725    0.001666  0.705774   \n",
       "ensemble_rf_xgb                           0.708157    0.001912  0.702935   \n",
       "xgb_feat_eng_incl_nums                    0.701238    0.002695  0.691590   \n",
       "feat_eng_model_lemma_clean                0.692113    0.001128  0.684044   \n",
       "feat_eng_model_lemma_fix                  0.621357    0.000901  0.642201   \n",
       "feat_eng_model                            0.618400    0.001553  0.640434   \n",
       "ensemble_rf_xgb_cos_sim                   0.612827    0.009669  0.636128   \n",
       "lstm_dropout_50                           0.594510    0.000000  0.638877   \n",
       "lstm_LEMMA_dropout_20_lstm_layer_DO_20    0.557914    0.000000  0.624349   \n",
       "lstm_mvp                                  0.595133    0.000000  0.637183   \n",
       "cos_sim_tfidf_model                       0.547188    0.001744  0.599010   \n",
       "lstm_dropout_50_lstm_layer_DO_50          0.578080    0.000000  0.618360   \n",
       "lstm_dropout50_dense50_BatchNorm          0.494492    0.000000  0.573654   \n",
       "cos_sim_model                             0.446336    0.002215  0.532120   \n",
       "mvp (tf-idf, nmf(5), xgboost)             0.385736    0.002493  0.487325   \n",
       "mvp (+ lemma)                             0.387424    0.003230  0.485464   \n",
       "\n",
       "                                          std_f1   avg_auc   std_auc  \\\n",
       "rf_feat_eng_model_lemma_clean           0.002658  0.868202  0.001148   \n",
       "ensemble_rf_xgb                         0.003148  0.863334  0.001438   \n",
       "xgb_feat_eng_incl_nums                  0.001899  0.851957  0.001192   \n",
       "feat_eng_model_lemma_clean              0.002549  0.846923  0.001643   \n",
       "feat_eng_model_lemma_fix                0.001609  0.822197  0.001710   \n",
       "feat_eng_model                          0.002281  0.821070  0.001428   \n",
       "ensemble_rf_xgb_cos_sim                 0.009994  0.819987  0.005193   \n",
       "lstm_dropout_50                         0.000000  0.802315  0.000000   \n",
       "lstm_LEMMA_dropout_20_lstm_layer_DO_20  0.000000  0.801938  0.000000   \n",
       "lstm_mvp                                0.000000  0.801019  0.000000   \n",
       "cos_sim_tfidf_model                     0.001703  0.800271  0.001291   \n",
       "lstm_dropout_50_lstm_layer_DO_50        0.000000  0.785850  0.000000   \n",
       "lstm_dropout50_dense50_BatchNorm        0.000000  0.772344  0.000000   \n",
       "cos_sim_model                           0.001306  0.746769  0.001279   \n",
       "mvp (tf-idf, nmf(5), xgboost)           0.001983  0.740593  0.001647   \n",
       "mvp (+ lemma)                           0.002485  0.738037  0.001362   \n",
       "\n",
       "                                        avg_log_loss  std_log_loss  \n",
       "rf_feat_eng_model_lemma_clean               0.436197      0.000640  \n",
       "ensemble_rf_xgb                             0.441784      0.001107  \n",
       "xgb_feat_eng_incl_nums                      0.450099      0.001675  \n",
       "feat_eng_model_lemma_clean                  0.456929      0.001410  \n",
       "feat_eng_model_lemma_fix                    0.488131      0.001342  \n",
       "feat_eng_model                              0.489465      0.001141  \n",
       "ensemble_rf_xgb_cos_sim                     0.493703      0.003901  \n",
       "lstm_dropout_50                             8.570912      0.000000  \n",
       "lstm_LEMMA_dropout_20_lstm_layer_DO_20      8.561581      0.000000  \n",
       "lstm_mvp                                    8.643059      0.000000  \n",
       "cos_sim_tfidf_model                         0.512085      0.001299  \n",
       "lstm_dropout_50_lstm_layer_DO_50            9.099757      0.000000  \n",
       "lstm_dropout50_dense50_BatchNorm            9.373478      0.000000  \n",
       "cos_sim_model                               0.565250      0.000963  \n",
       "mvp (tf-idf, nmf(5), xgboost)               0.568958      0.001288  \n",
       "mvp (+ lemma)                               0.572483      0.000815  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = utils.load('results')\n",
    "\n",
    "results_df = results_df.drop(index=model_name, errors='ignore')\n",
    "results_df = results_df.append(utils.log_keras_scores(label_val, y_prob, model_name))\n",
    "results_df.sort_values('avg_auc', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save(results_df, 'results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "Similar to the best NN model yet. Let's build upon this model.\n",
    "\n",
    "* Remove stop words\n",
    "* Add BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
