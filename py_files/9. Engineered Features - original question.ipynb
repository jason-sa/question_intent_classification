{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced featrue engineer model\n",
    "\n",
    "This model will add engineered features for the original question, in addition to the lemmatized question.\n",
    "\n",
    "Engineered two different types of features,\n",
    "\n",
    "1. n_gram similarity between each pair of questions\n",
    "2. min/max/avg distance between words in a single question. Currently using the following metrics,\n",
    "  * euclidean\n",
    "  * cosine\n",
    "  * city block or manhattan\n",
    "  \n",
    "**Pipeline**\n",
    "1. Stack questions\n",
    "2. Clean questions - now lower cases all words to better lemmatize proper nouns\n",
    "3. UNION\n",
    "    1. n_gram similarity\n",
    "    2. min/max/avg distance\n",
    "4. Lemmatize questions\n",
    "5. UNION\n",
    "    1. n_gram similarity\n",
    "    2. min/max/avg distances\n",
    "6. UNION together both sets of features\n",
    "7. XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T04:57:04.646711Z",
     "start_time": "2018-11-28T04:56:47.834474Z"
    }
   },
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# modeling\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T04:57:05.108061Z",
     "start_time": "2018-11-28T04:57:04.649533Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = utils.load('X_train')\n",
    "y_train = utils.load('y_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T04:57:05.128186Z",
     "start_time": "2018-11-28T04:57:05.110672Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature engineering pipes\n",
    "single_question_pipe = Pipeline(\n",
    "    [\n",
    "        ('dist', FunctionTransformer(utils.add_min_max_avg_distance_features, validate=False)),\n",
    "        ('unstack', FunctionTransformer(utils.unstack_questions, validate=False))\n",
    "    ]\n",
    ")\n",
    "\n",
    "pair_question_pipe = Pipeline(\n",
    "    [\n",
    "        ('ngram_sim', FunctionTransformer(utils.calc_ngram_similarity, kw_args={'n_grams':[1, 2, 3]}, validate=False))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# clean text pipe\n",
    "clean_text_pipe = Pipeline(\n",
    "    [\n",
    "        ('stack', FunctionTransformer(utils.stack_questions, validate=False)),\n",
    "        ('clean', FunctionTransformer(utils.clean_questions, validate=False)),\n",
    "        ('feats', FeatureUnion(\n",
    "            [\n",
    "                ('pair', pair_question_pipe),\n",
    "                ('single', single_question_pipe)\n",
    "            ]\n",
    "        ))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# lemma pipe\n",
    "lemma_pipe = Pipeline(\n",
    "    [\n",
    "        ('stack', FunctionTransformer(utils.stack_questions, validate=False)),\n",
    "        ('clean', FunctionTransformer(utils.clean_questions, validate=False)),\n",
    "        ('lemma', FunctionTransformer(utils.apply_lemma, validate=False)),\n",
    "        ('feats', FeatureUnion(\n",
    "            [\n",
    "                ('pair', pair_question_pipe),\n",
    "                ('single', single_question_pipe)\n",
    "            ]\n",
    "        ))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# pre-process pipe\n",
    "pre_process_pipe = Pipeline(\n",
    "    [\n",
    "        ('feats', FeatureUnion(\n",
    "            [\n",
    "                ('clean_features', clean_text_pipe),\n",
    "                ('lemma_pipe', lemma_pipe)\n",
    "            ]\n",
    "        ))\n",
    "    ]\n",
    ")\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=500, n_jobs=-1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T04:57:09.161680Z",
     "start_time": "2018-11-28T04:57:05.134427Z"
    }
   },
   "outputs": [],
   "source": [
    "X_transform = pre_process_pipe.transform(X_train)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3, random_state=42)\n",
    "cv = cross_validate(xgb, \n",
    "               X_transform, \n",
    "               y_train, \n",
    "               cv=skf, \n",
    "               n_jobs=1, \n",
    "               scoring=('accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'neg_log_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_accuracy</th>\n",
       "      <th>std_accuracy</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>std_precision</th>\n",
       "      <th>avg_recall</th>\n",
       "      <th>std_recall</th>\n",
       "      <th>avg_f1</th>\n",
       "      <th>std_f1</th>\n",
       "      <th>avg_auc</th>\n",
       "      <th>std_auc</th>\n",
       "      <th>avg_log_loss</th>\n",
       "      <th>std_log_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mvp (tf-idf, nmf(5), xgboost)</th>\n",
       "      <td>0.700345</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.661571</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.385736</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>0.487325</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.740593</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.568958</td>\n",
       "      <td>0.001288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mvp (+ lemma)</th>\n",
       "      <td>0.696787</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.649977</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.387424</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>0.485464</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>0.738037</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.572483</td>\n",
       "      <td>0.000815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cos_sim_model</th>\n",
       "      <td>0.710200</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.658748</td>\n",
       "      <td>0.002578</td>\n",
       "      <td>0.446336</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>0.532120</td>\n",
       "      <td>0.001306</td>\n",
       "      <td>0.746769</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.565250</td>\n",
       "      <td>0.000963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_eng_model</th>\n",
       "      <td>0.743614</td>\n",
       "      <td>0.002021</td>\n",
       "      <td>0.664102</td>\n",
       "      <td>0.003502</td>\n",
       "      <td>0.618400</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>0.640434</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>0.821070</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.489465</td>\n",
       "      <td>0.001141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_eng_model_lemma_fix</th>\n",
       "      <td>0.744356</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>0.664513</td>\n",
       "      <td>0.004333</td>\n",
       "      <td>0.621357</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.642201</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>0.822197</td>\n",
       "      <td>0.001710</td>\n",
       "      <td>0.488131</td>\n",
       "      <td>0.001342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_feat_eng_model_lemma_clean</th>\n",
       "      <td>0.783667</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>0.708853</td>\n",
       "      <td>0.003681</td>\n",
       "      <td>0.702725</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>0.705774</td>\n",
       "      <td>0.002658</td>\n",
       "      <td>0.868202</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.436197</td>\n",
       "      <td>0.000640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensemble_rf_xgb</th>\n",
       "      <td>0.779000</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.697794</td>\n",
       "      <td>0.004357</td>\n",
       "      <td>0.708157</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.702935</td>\n",
       "      <td>0.003148</td>\n",
       "      <td>0.863334</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.441784</td>\n",
       "      <td>0.001107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cos_sim_tfidf_model</th>\n",
       "      <td>0.729511</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.661680</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.547188</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.599010</td>\n",
       "      <td>0.001703</td>\n",
       "      <td>0.800271</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>0.512085</td>\n",
       "      <td>0.001299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_eng_model_lemma_clean</th>\n",
       "      <td>0.763927</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.676166</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>0.692113</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.684044</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.846923</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>0.456929</td>\n",
       "      <td>0.001410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               avg_accuracy  std_accuracy  avg_precision  \\\n",
       "mvp (tf-idf, nmf(5), xgboost)      0.700345      0.000466       0.661571   \n",
       "mvp (+ lemma)                      0.696787      0.001055       0.649977   \n",
       "cos_sim_model                      0.710200      0.000830       0.658748   \n",
       "feat_eng_model                     0.743614      0.002021       0.664102   \n",
       "feat_eng_model_lemma_fix           0.744356      0.002107       0.664513   \n",
       "rf_feat_eng_model_lemma_clean      0.783667      0.002260       0.708853   \n",
       "ensemble_rf_xgb                    0.779000      0.002740       0.697794   \n",
       "cos_sim_tfidf_model                0.729511      0.001216       0.661680   \n",
       "feat_eng_model_lemma_clean         0.763927      0.002404       0.676166   \n",
       "\n",
       "                               std_precision  avg_recall  std_recall  \\\n",
       "mvp (tf-idf, nmf(5), xgboost)       0.000461    0.385736    0.002493   \n",
       "mvp (+ lemma)                       0.003057    0.387424    0.003230   \n",
       "cos_sim_model                       0.002578    0.446336    0.002215   \n",
       "feat_eng_model                      0.003502    0.618400    0.001553   \n",
       "feat_eng_model_lemma_fix            0.004333    0.621357    0.000901   \n",
       "rf_feat_eng_model_lemma_clean       0.003681    0.702725    0.001666   \n",
       "ensemble_rf_xgb                     0.004357    0.708157    0.001912   \n",
       "cos_sim_tfidf_model                 0.002219    0.547188    0.001744   \n",
       "feat_eng_model_lemma_clean          0.003904    0.692113    0.001128   \n",
       "\n",
       "                                 avg_f1    std_f1   avg_auc   std_auc  \\\n",
       "mvp (tf-idf, nmf(5), xgboost)  0.487325  0.001983  0.740593  0.001647   \n",
       "mvp (+ lemma)                  0.485464  0.002485  0.738037  0.001362   \n",
       "cos_sim_model                  0.532120  0.001306  0.746769  0.001279   \n",
       "feat_eng_model                 0.640434  0.002281  0.821070  0.001428   \n",
       "feat_eng_model_lemma_fix       0.642201  0.001609  0.822197  0.001710   \n",
       "rf_feat_eng_model_lemma_clean  0.705774  0.002658  0.868202  0.001148   \n",
       "ensemble_rf_xgb                0.702935  0.003148  0.863334  0.001438   \n",
       "cos_sim_tfidf_model            0.599010  0.001703  0.800271  0.001291   \n",
       "feat_eng_model_lemma_clean     0.684044  0.002549  0.846923  0.001643   \n",
       "\n",
       "                               avg_log_loss  std_log_loss  \n",
       "mvp (tf-idf, nmf(5), xgboost)      0.568958      0.001288  \n",
       "mvp (+ lemma)                      0.572483      0.000815  \n",
       "cos_sim_model                      0.565250      0.000963  \n",
       "feat_eng_model                     0.489465      0.001141  \n",
       "feat_eng_model_lemma_fix           0.488131      0.001342  \n",
       "rf_feat_eng_model_lemma_clean      0.436197      0.000640  \n",
       "ensemble_rf_xgb                    0.441784      0.001107  \n",
       "cos_sim_tfidf_model                0.512085      0.001299  \n",
       "feat_eng_model_lemma_clean         0.456929      0.001410  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = utils.load('results')\n",
    "\n",
    "results_df = results_df.drop(index='feat_eng_model_lemma_clean', errors='ignore')\n",
    "results_df = results_df.append(utils.log_scores(cv, 'feat_eng_model_lemma_clean'))\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save(results_df, 'results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Wow! The feature engineering shows a significant jump in AUC from 0.8 to 0.82."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gt</th>\n",
       "      <th>prob</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.393622</td>\n",
       "      <td>-0.393622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.483499</td>\n",
       "      <td>-0.483499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.230031</td>\n",
       "      <td>-0.230031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>-0.000473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.027378</td>\n",
       "      <td>-0.027378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gt      prob      diff\n",
       "0   0  0.393622 -0.393622\n",
       "1   0  0.483499 -0.483499\n",
       "2   0  0.230031 -0.230031\n",
       "3   0  0.000473 -0.000473\n",
       "4   0  0.027378 -0.027378"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.fit(X_transform, y_train)\n",
    "utils.save(xgb, 'xgb_feat_eng_model')\n",
    "\n",
    "y_probs = xgb.predict_proba(X_transform)[:, 1]\n",
    "class_errors_df = utils.ground_truth_analysis(y_train, y_probs)\n",
    "class_errors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_pipe = Pipeline(\n",
    "    [\n",
    "        ('stack', FunctionTransformer(utils.stack_questions, validate=False)),\n",
    "        ('clean', FunctionTransformer(utils.clean_questions, validate=False)),\n",
    "        ('lemma', FunctionTransformer(utils.apply_lemma, validate=False)),\n",
    "    ]\n",
    ")\n",
    "X_train_lemma = lemma_pipe.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top false negative errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob: 0.001027111\n",
      "\n",
      "Is this move of banning 500 & 1000 Rupee notes right?\n",
      "What's Balaji Vishwanathan's take on banning 500 and 1000 Rs. currency?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "ban  rupee note right\n",
      "s balaji vishwanathan ban   rs currency\n",
      "\n",
      "Feature Space------\n",
      "[1.00000000e-01 0.00000000e+00 0.00000000e+00 4.20957322e+00\n",
      " 1.07570888e+01 7.72370876e+00 3.34027115e-01 1.19045726e+00\n",
      " 8.35664798e-01 5.76906718e+01 1.45533761e+02 1.03886527e+02\n",
      " 4.46231557e+00 9.78955353e+00 7.04691247e+00 3.60066705e-01\n",
      " 9.95033675e-01 7.27225144e-01 6.11183202e+01 1.32570303e+02\n",
      " 9.57032626e+01 1.66666667e-01 0.00000000e+00 0.00000000e+00\n",
      " 7.22822451e+00 1.07570888e+01 9.17083311e+00 7.78746179e-01\n",
      " 1.10782110e+00 9.21600294e-01 9.70216655e+01 1.45533761e+02\n",
      " 1.23956212e+02 5.51670254e+00 9.80649784e+00 8.05885513e+00\n",
      " 5.78655325e-01 9.95033675e-01 8.13570311e-01 7.30594916e+01\n",
      " 1.30713938e+02 1.08378265e+02]\n",
      "-------------------------------------------\n",
      "\n",
      "Prob: 0.0016096202\n",
      "\n",
      "Is there any Nano technology GPS tracking features in new 500 & 2000 rupee notes to be released by Reserve Bank of India?\n",
      "Are the new 500 rs and 2000 rs Indian currency notes really chip-based?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "new  rs  rs indian currency note chipbas\n",
      "nano technology gps tracking feature new  rupee note release reserve bank india\n",
      "\n",
      "Feature Space------\n",
      "[1.25000000e-01 0.00000000e+00 0.00000000e+00 3.74563779e+00\n",
      " 1.03951500e+01 7.49422591e+00 2.87047854e-01 1.02704093e+00\n",
      " 7.56264642e-01 5.21136331e+01 1.42736224e+02 1.01741125e+02\n",
      " 0.00000000e+00 9.49674949e+00 7.33800235e+00 0.00000000e+00\n",
      " 9.98488933e-01 7.61702640e-01 0.00000000e+00 1.29213141e+02\n",
      " 9.95157481e+01 3.00000000e-01 9.52380952e-02 0.00000000e+00\n",
      " 5.80027399e+00 1.03951500e+01 8.17089806e+00 5.28701050e-01\n",
      " 1.00172643e+00 7.88341432e-01 8.10819889e+01 1.42736224e+02\n",
      " 1.11216348e+02 0.00000000e+00 9.47690319e+00 7.60485511e+00\n",
      " 0.00000000e+00 9.60178703e-01 7.72467963e-01 0.00000000e+00\n",
      " 1.29213141e+02 1.03284869e+02]\n",
      "-------------------------------------------\n",
      "\n",
      "Prob: 0.002783158\n",
      "\n",
      "Can a person increase his/her height even after 20?\n",
      "Are there any chances of growing height at the age of 20?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "chance grow height age\n",
      "person increase hisher height\n",
      "\n",
      "Feature Space------\n",
      "[1.11111111e-01 0.00000000e+00 0.00000000e+00 3.87716632e+00\n",
      " 8.12650779e+00 6.29732511e+00 2.98833571e-01 8.02507465e-01\n",
      " 5.90768817e-01 5.20895255e+01 1.10694358e+02 8.57181337e+01\n",
      " 0.00000000e+00 8.36861715e+00 6.15836505e+00 0.00000000e+00\n",
      " 8.49555119e-01 6.02074332e-01 0.00000000e+00 1.14582147e+02\n",
      " 8.43705847e+01 2.50000000e-01 0.00000000e+00 0.00000000e+00\n",
      " 7.25347294e+00 8.12650779e+00 7.65189503e+00 6.50877228e-01\n",
      " 7.62674892e-01 6.97757593e-01 1.00611163e+02 1.10694358e+02\n",
      " 1.05309908e+02 6.71474805e+00 8.16892091e+00 7.46108825e+00\n",
      " 6.26050272e-01 8.65093922e-01 6.87849115e-01 9.36091349e+01\n",
      " 1.10192096e+02 1.02208312e+02]\n",
      "-------------------------------------------\n",
      "\n",
      "Prob: 0.0029385053\n",
      "\n",
      "Psychotherapy: How do I find an individual psychotherapist?\n",
      "Psychotherapy: What is the Yelp for therapists?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "psychotherapy find individual psychotherapist\n",
      "psychotherapy yelp therapist\n",
      "\n",
      "Feature Space------\n",
      "[  0.13333333   0.           0.           4.40340196   9.77071577\n",
      "   7.41906187   0.3880387    1.02532067   0.76650935  60.934365\n",
      " 132.28646036 100.87777248   4.30849117   9.57030104   7.11122005\n",
      "   0.31031856   0.97532099   0.68335408  58.54252309 123.89788954\n",
      "  95.86871401   0.28571429   0.           0.           6.41429258\n",
      "   9.75133165   8.50562646   0.39222232   0.91367898   0.73250502\n",
      "  88.51778327 132.00748627 116.05904376   5.92354825   8.54363907\n",
      "   7.59449237   0.3588007    0.90145333   0.70946777  78.57286128\n",
      " 116.28954342 102.9581444 ]\n",
      "-------------------------------------------\n",
      "\n",
      "Prob: 0.003052021\n",
      "\n",
      "Which is your favourite film in 2016?\n",
      "What are the best movies of 2016?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "good movie\n",
      "favourite film\n",
      "\n",
      "Feature Space------\n",
      "[  0.           0.           0.           3.69345477   9.16082649\n",
      "   6.71822443   0.28109422   0.87034687   0.65727368  49.12058941\n",
      " 125.33448875  91.11492373   3.67209771   7.90194021   5.92038645\n",
      "   0.28606134   0.77292046   0.56194962  49.32912163 107.25735693\n",
      "  80.61673282   0.           0.           0.           8.27739413\n",
      "   8.27739413   8.27739413   0.71539839   0.71539839   0.71539839\n",
      " 115.3893874  115.3893874  115.3893874    7.42813575   7.42813575\n",
      "   7.42813575   0.6347761    0.6347761    0.6347761  100.33431423\n",
      " 100.33431423 100.33431423]\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fn_idx = class_errors_df.sort_values('diff', ascending = False).head().index\n",
    "for idx in fn_idx:\n",
    "    print('Prob:', y_probs[idx])\n",
    "    print()\n",
    "    print(X_train.iloc[idx].question1)\n",
    "    print(X_train.iloc[idx].question2)\n",
    "    print()\n",
    "    print('Lemma--------')\n",
    "    print()\n",
    "    print(X_train_lemma[idx*2])\n",
    "    print(X_train_lemma[idx*2+1])\n",
    "    print()\n",
    "    print('Feature Space------')\n",
    "    print(X_transform[idx])\n",
    "    print('-------------------------------------------')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top false positive errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob: 0.9608003\n",
      "\n",
      "Why do people ask questions on Quora that can easily be answered by Google?\n",
      "Why do people use Quora to ask questions when Google or Wikipedia would be sufficient?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "people use quora ask question google wikipedia sufficient\n",
      "people ask question quora easily answer google\n",
      "\n",
      "Feature Space------\n",
      "[5.51724138e-01 2.06896552e-01 6.89655172e-02 3.30257131e+00\n",
      " 1.05098422e+01 6.48271009e+00 2.22047507e-01 1.25827333e+00\n",
      " 6.44444557e-01 4.56209202e+01 1.38667841e+02 8.77513434e+01\n",
      " 3.35628163e+00 1.00165953e+01 6.17317732e+00 2.23288568e-01\n",
      " 1.25827333e+00 6.11561284e-01 4.57775741e+01 1.30679687e+02\n",
      " 8.26270183e+01 6.66666667e-01 1.33333333e-01 0.00000000e+00\n",
      " 3.54939540e+00 1.02873433e+01 6.97978315e+00 2.03129739e-01\n",
      " 1.19576709e+00 6.79430514e-01 4.94086112e+01 1.38667841e+02\n",
      " 9.37634244e+01 4.57339153e+00 1.00165953e+01 7.14255837e+00\n",
      " 3.35699168e-01 1.14565285e+00 7.17501372e-01 6.20160041e+01\n",
      " 1.29971439e+02 9.49136603e+01]\n",
      "-------------------------------------------\n",
      "\n",
      "Prob: 0.941164\n",
      "\n",
      "Which is best laptop under 40k?\n",
      "Which is the best laptop under 50k?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "good laptop\n",
      "good laptop\n",
      "\n",
      "Feature Space------\n",
      "[  0.90909091   0.72727273   0.54545455   3.69345477   8.66277392\n",
      "   6.56414204   0.28109422   0.81015074   0.63860738  49.12058941\n",
      " 116.8873165   90.22042004   3.69345477   8.66277392   6.19443128\n",
      "   0.28109422   0.81328662   0.60171475  49.12058941 116.8873165\n",
      "  85.16521773   1.           1.           1.           8.11010119\n",
      "   8.11010119   8.11010119   0.74249538   0.74249538   0.74249538\n",
      " 109.55759236 109.55759236 109.55759236   8.11010119   8.11010119\n",
      "   8.11010119   0.74249538   0.74249538   0.74249538 109.55759236\n",
      " 109.55759236 109.55759236]\n",
      "-------------------------------------------\n",
      "\n",
      "Prob: 0.92863625\n",
      "\n",
      "Why do really specific questions on Quora always get marked as needing clarification?\n",
      "Why are all my questions marked as needing clarification in Quora?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "specific question quora mark need clarification\n",
      "question mark need clarification quora\n",
      "\n",
      "Feature Space------\n",
      "[  0.58333333   0.25         0.16666667   4.31420649  10.15199955\n",
      "   6.61170542   0.34403003   1.2691362    0.72518658  57.778146\n",
      " 133.1045724   89.09010392   3.20201303   9.9450952    6.29727827\n",
      "   0.18023726   1.25561417   0.66535907  44.43411171 132.12559452\n",
      "  84.31663362   0.90909091   0.36363636   0.18181818   5.12222342\n",
      "   9.86289191   7.53653975   0.43171622   1.15707429   0.80366789\n",
      "  67.65169716 129.14824565 100.38534803   5.12222342   9.86289191\n",
      "   7.27010659   0.43171622   1.15707429   0.75967505  67.65169716\n",
      " 129.14824565  96.25296504]\n",
      "-------------------------------------------\n",
      "\n",
      "Prob: 0.92651814\n",
      "\n",
      "What are the safety precautions on handling shotguns proposed by the NRA in Iowa?\n",
      "What are the safety precautions on handling shotguns proposed by the NRA in Alabama and every other state, territory and possession in the U.S.?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "safety precaution handle shotgun propose nra alabama state territory possession\n",
      "safety precaution handle shotgun propose nra iowa\n",
      "\n",
      "Feature Space------\n",
      "[  0.72727273   0.63157895   0.57894737   0.          10.07600953\n",
      "   7.45883891   0.           1.12346169   0.76835582   0.\n",
      " 134.76468542 101.09182697   0.          10.07600953   6.84279795\n",
      "   0.           1.12346169   0.6953473    0.         134.76468542\n",
      "  93.03279626   0.70588235   0.58823529   0.47058824   6.29423465\n",
      "   9.34155072   8.26775547   0.48500935   0.9850978    0.81127976\n",
      "  86.42554037 124.73015501 112.94254688   6.29423465   9.90783646\n",
      "   8.20914772   0.48500935   1.00067223   0.79623999  86.42554037\n",
      " 131.39232499 111.6539695 ]\n",
      "-------------------------------------------\n",
      "\n",
      "Prob: 0.92651814\n",
      "\n",
      "What are the safety precautions on handling shotguns proposed by the NRA in Colorado?\n",
      "What are the safety precautions on handling shotguns proposed by the NRA in Alabama and every other state, territory and possession in the U.S.?\n",
      "\n",
      "Lemma--------\n",
      "\n",
      "safety precaution handle shotgun propose nra alabama state territory possession\n",
      "safety precaution handle shotgun propose nra colorado\n",
      "\n",
      "Feature Space------\n",
      "[  0.72727273   0.63157895   0.57894737   0.          10.07600953\n",
      "   7.46096068   0.           1.12346169   0.76625584   0.\n",
      " 134.76468542 101.09255859   0.          10.07600953   6.84279795\n",
      "   0.           1.12346169   0.6953473    0.         134.76468542\n",
      "  93.03279626   0.70588235   0.58823529   0.47058824   6.29423465\n",
      "   9.34155072   8.28864      0.48500935   0.9850978    0.8108802\n",
      "  86.42554037 125.13493604 113.3682471    6.29423465   9.90783646\n",
      "   8.20914772   0.48500935   1.00067223   0.79623999  86.42554037\n",
      " 131.39232499 111.6539695 ]\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fn_idx = class_errors_df.sort_values('diff').head().index\n",
    "for idx in fn_idx:\n",
    "    print('Prob:', y_probs[idx])\n",
    "    print()\n",
    "    print(X_train.iloc[idx].question1)\n",
    "    print(X_train.iloc[idx].question2)\n",
    "    print()\n",
    "    print('Lemma--------')\n",
    "    print()\n",
    "    print(X_train_lemma[idx*2])\n",
    "    print(X_train_lemma[idx*2+1])\n",
    "    print()\n",
    "    print('Feature Space------')\n",
    "    print(X_transform[idx])\n",
    "    print('-------------------------------------------')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. The false negative errors appear to be related to numbers. \n",
    "  * Either stop removing numebers in the clean text proces, or create another set of features.\n",
    "2. The false positives seem very tricky. Found the Facebook InferSent model which embeds sentences to 4096 dimension.\n",
    "  * Could see if the distances between some of the false positive examples is different in this space.\n",
    "3. Could look at alignments of the two questions. (I think nltk)\n",
    "4. Add different word embeddings to get a different nuance, or even use the vector only data set from Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
