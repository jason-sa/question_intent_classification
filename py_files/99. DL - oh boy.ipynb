{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:05:40.872824Z",
     "start_time": "2018-11-29T00:05:40.835246Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# data manipulation\n",
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Input, Add, Concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "# from keras import backend as K\n",
    "\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:54:40.060662Z",
     "start_time": "2018-11-28T23:54:39.617510Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = utils.load('X_train')\n",
    "y_train = utils.load('y_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Encode vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:54:48.602373Z",
     "start_time": "2018-11-28T23:54:40.063608Z"
    }
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 20000\n",
    "X_train_stack = utils.clean_questions(utils.stack_questions(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:55:20.318792Z",
     "start_time": "2018-11-28T23:54:48.606183Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(X_train_stack)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(X_train_stack)\n",
    "data = pad_sequences(sequences, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:55:20.328275Z",
     "start_time": "2018-11-28T23:55:20.321523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(606398, 100)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:55:20.352546Z",
     "start_time": "2018-11-28T23:55:20.332045Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7212"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:,0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limited the questions to 100 words. \n",
    "\n",
    "**Note to self**\n",
    "* Lemmatization transformation could help\n",
    "* Not removing numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:59:00.965247Z",
     "start_time": "2018-11-28T23:55:20.356138Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocabulary_size, 300))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "#     print(word, index, end='\\r')\n",
    "    if index > vocabulary_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = utils.nlp(word).vector\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "#     break\n",
    "\n",
    "utils.save(embedding_matrix, 'embedding_matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build out legs of the siamese network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:12:16.352145Z",
     "start_time": "2018-11-29T00:12:16.302955Z"
    }
   },
   "source": [
    "``` python\n",
    "input_shape = (100,)\n",
    "input_l = Input(input_shape)\n",
    "input_r = Input(input_shape)\n",
    "\n",
    "# conv_lstm_net = Sequential()\n",
    "conv_lstm_net = Embedding(vocabulary_size, 300, input_length=100, weights=[embedding_matrix], trainable=False)\n",
    "conv_lstm_net.add(Dropout(0.2))\n",
    "conv_lstm_net.add(Conv1D(64, 5, activation='relu'))\n",
    "conv_lstm_net.add(MaxPooling1D(pool_size=4))\n",
    "conv_lstm_net.add(LSTM(300))\n",
    "conv_lstm_net.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "encoded_l = conv_lstm_net(input_l)\n",
    "encoded_r = conv_lstm_net(input_r)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:07:09.765140Z",
     "start_time": "2018-11-29T00:07:09.680279Z"
    }
   },
   "source": [
    "``` python\n",
    "# L1_distance = lambda x: K.abs(x[0]-x[1])\n",
    "# siamese_net = Sequential()\n",
    "# siamese_net.add(Concatenate([encoded_l, encoded_r]))\n",
    "# siamese_net.add(Dense(1,activation='sigmoid'))\n",
    "# both = merge()\n",
    "# both = concatenate([encoded_l,encoded_r], mode = L1_distance, output_shape=lambda x: x[0])\n",
    "# prediction = Dense(1,activation='sigmoid')(both)\n",
    "siamese_net = Model(inputs=[encoded_l,encoded_r],outputs=prediction)\n",
    "SVG(model_to_dot(siamese_net, show_shapes=True).create(prog='dot', format='svg'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:15:21.951772Z",
     "start_time": "2018-11-29T00:15:19.924658Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating word embedding layer\n",
    "embedding_layer = Embedding(vocabulary_size, 300, input_length=100, \n",
    "                                     weights=[embedding_matrix], trainable=False)\n",
    "\n",
    "# Creating LSTM Encoder\n",
    "# Bidirectional(LSTM(self.number_lstm_units, dropout=self.rate_drop_lstm, recurrent_dropout=self.rate_drop_lstm))\n",
    "lstm_layer = LSTM(300)\n",
    "\n",
    "# Creating LSTM Encoder layer for First Sentence\n",
    "sequence_1_input = Input(shape=(100,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "# Creating LSTM Encoder layer for Second Sentence\n",
    "sequence_2_input = Input(shape=(100,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "x2 = lstm_layer(embedded_sequences_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:16:12.624956Z",
     "start_time": "2018-11-29T00:16:12.372819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 300)     6000000     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 300)          721200      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 600)          0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            601         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 6,721,801\n",
      "Trainable params: 721,801\n",
      "Non-trainable params: 6,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import concatenate\n",
    "# Merging two LSTM encodes vectors from sentences to\n",
    "# pass it to dense layer applying dropout and batch normalisation\n",
    "merged = concatenate([x1, x2])\n",
    "# merged = BatchNormalization()(merged)\n",
    "# merged = Dropout(self.rate_drop_dense)(merged)\n",
    "# merged = Dense(self.number_dense_units, activation=self.activation_function)(merged)\n",
    "# merged = BatchNormalization()(merged)\n",
    "# merged = Dropout(self.rate_drop_dense)(merged)\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "# SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the batch to pass into the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:25:57.617365Z",
     "start_time": "2018-11-29T00:25:57.429291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train neg class: 0.63\n",
      "Val neg class: 0.61\n"
     ]
    }
   ],
   "source": [
    "odd_idx = [i for i in range(data.shape[0]) if i % 2 == 1]\n",
    "even_idx = [i for i in range(data.shape[0]) if i % 2 == 0]\n",
    "\n",
    "train_data_x1 = data[odd_idx[:10000]]\n",
    "train_data_x2 = data[even_idx[:10000]]\n",
    "train_labels = y_train[:10000]\n",
    "\n",
    "val_data_x1 = data[odd_idx[10000:11000]]\n",
    "val_data_x2 = data[even_idx[10000:11000]]\n",
    "val_labels = y_train[10000:11000]\n",
    "\n",
    "print(f'Train neg class: {len(train_labels[train_labels == 0]) / len(train_labels):.2}')\n",
    "print(f'Val neg class: {len(val_labels[val_labels == 0]) / len(val_labels):.2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-29T00:26:39.525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 86s 9ms/step - loss: 0.6226 - acc: 0.6648 - val_loss: 0.6235 - val_acc: 0.6700\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 74s 7ms/step - loss: 0.6108 - acc: 0.6914 - val_loss: 0.6161 - val_acc: 0.6600\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 70s 7ms/step - loss: 0.5608 - acc: 0.7164 - val_loss: 0.5872 - val_acc: 0.6820\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 72s 7ms/step - loss: 0.4878 - acc: 0.7668 - val_loss: 0.5715 - val_acc: 0.7070\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 67s 7ms/step - loss: 0.4059 - acc: 0.8188 - val_loss: 0.6005 - val_acc: 0.7030\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 0.2948 - acc: 0.8823 - val_loss: 0.7115 - val_acc: 0.6740\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 0.1691 - acc: 0.9439 - val_loss: 0.8075 - val_acc: 0.6840\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: 0.0809 - acc: 0.9779 - val_loss: 0.9791 - val_acc: 0.6900\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: 0.0408 - acc: 0.9911 - val_loss: 1.0932 - val_acc: 0.7020\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 69s 7ms/step - loss: 0.0211 - acc: 0.9966 - val_loss: 1.2171 - val_acc: 0.6990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6850338080>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([train_data_x1, train_data_x2], train_labels,\n",
    "                  validation_data=([val_data_x1, val_data_x2], val_labels),\n",
    "                  epochs=10, batch_size=64, shuffle=True)\n",
    "# ,\n",
    "#                   callbacks=[early_stopping, model_checkpoint, tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfit like crazy, so need to add drop out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:16:12.624956Z",
     "start_time": "2018-11-29T00:16:12.372819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 300)     6000000     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 300)          721200      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 600)          0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 600)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            601         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,721,801\n",
      "Trainable params: 721,801\n",
      "Non-trainable params: 6,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Merging two LSTM encodes vectors from sentences to\n",
    "# pass it to dense layer applying dropout and batch normalisation\n",
    "merged = concatenate([x1, x2])\n",
    "# merged = BatchNormalization()(merged)\n",
    "merged = Dropout(.2)(merged)\n",
    "# merged = Dense(self.number_dense_units, activation=self.activation_function)(merged)\n",
    "# merged = BatchNormalization()(merged)\n",
    "# merged = Dropout(self.rate_drop_dense)(merged)\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "# SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-29T00:26:39.525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 50s 5ms/step - loss: 0.6197 - acc: 0.6630 - val_loss: 0.7178 - val_acc: 0.6280\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 47s 5ms/step - loss: 0.5681 - acc: 0.7075 - val_loss: 0.5770 - val_acc: 0.6930\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 47s 5ms/step - loss: 0.5933 - acc: 0.6964 - val_loss: 0.5643 - val_acc: 0.7150\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 47s 5ms/step - loss: 0.4830 - acc: 0.7711 - val_loss: 0.5850 - val_acc: 0.6950\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 47s 5ms/step - loss: 0.4037 - acc: 0.8182 - val_loss: 0.6329 - val_acc: 0.7010\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 47s 5ms/step - loss: 0.3085 - acc: 0.8696 - val_loss: 0.6978 - val_acc: 0.6600\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.2026 - acc: 0.9235 - val_loss: 0.7558 - val_acc: 0.7130\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 47s 5ms/step - loss: 0.1846 - acc: 0.9310 - val_loss: 0.8710 - val_acc: 0.6830\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.0902 - acc: 0.9743 - val_loss: 0.9897 - val_acc: 0.6780\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 47s 5ms/step - loss: 0.0453 - acc: 0.9891 - val_loss: 1.2270 - val_acc: 0.6890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7d503c0da0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([train_data_x1, train_data_x2], train_labels,\n",
    "                  validation_data=([val_data_x1, val_data_x2], val_labels),\n",
    "                  epochs=10, batch_size=64, shuffle=True)\n",
    "# ,\n",
    "#                   callbacks=[early_stopping, model_checkpoint, tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another overfit...Let's add another hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:16:12.624956Z",
     "start_time": "2018-11-29T00:16:12.372819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 300)     6000000     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 300)          721200      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 600)          0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 600)          0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 100)          60100       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 100)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            101         dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,781,401\n",
      "Trainable params: 781,401\n",
      "Non-trainable params: 6,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Merging two LSTM encodes vectors from sentences to\n",
    "# pass it to dense layer applying dropout and batch normalisation\n",
    "merged = concatenate([x1, x2])\n",
    "# merged = BatchNormalization()(merged)\n",
    "merged = Dropout(.2)(merged)\n",
    "merged = Dense(100, activation='tanh')(merged)\n",
    "# merged = BatchNormalization()(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "# SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-29T00:26:39.525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203143 samples, validate on 100056 samples\n",
      "Epoch 1/10\n",
      "203143/203143 [==============================] - 1634s 8ms/step - loss: 0.5164 - acc: 0.7487 - val_loss: 0.4928 - val_acc: 0.7602\n",
      "Epoch 2/10\n",
      "203143/203143 [==============================] - 1845s 9ms/step - loss: 0.4758 - acc: 0.7722 - val_loss: 0.4643 - val_acc: 0.7781\n",
      "Epoch 3/10\n",
      "203143/203143 [==============================] - 1094s 5ms/step - loss: 0.4144 - acc: 0.8126 - val_loss: 0.4581 - val_acc: 0.7869\n",
      "Epoch 4/10\n",
      " 11520/203143 [>.............................] - ETA: 14:24 - loss: 0.3517 - acc: 0.8458"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-fbf0a2144719>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit([data[odd_idx], data[even_idx]], y_train, validation_split=0.33,\n\u001b[0;32m----> 2\u001b[0;31m                   epochs=10, batch_size=64, shuffle=True)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# ,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#                   callbacks=[early_stopping, model_checkpoint, tensorboard])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit([data[odd_idx], data[even_idx]], y_train, validation_split=0.33,\n",
    "                  epochs=10, batch_size=64, shuffle=True)\n",
    "# ,\n",
    "#                   callbacks=[early_stopping, model_checkpoint, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303199/303199 [==============================] - 1060s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict([data[odd_idx], data[even_idx]], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9064708310728856"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.roc_auc_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3771203559003589"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.log_loss(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
