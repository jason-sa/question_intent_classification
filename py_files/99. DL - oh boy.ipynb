{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:05:40.872824Z",
     "start_time": "2018-11-29T00:05:40.835246Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# data manipulation\n",
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Input, Add, Concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "# from keras import backend as K\n",
    "\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:54:40.060662Z",
     "start_time": "2018-11-28T23:54:39.617510Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = utils.load('X_train')\n",
    "y_train = utils.load('y_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Encode vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:54:48.602373Z",
     "start_time": "2018-11-28T23:54:40.063608Z"
    }
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 20000\n",
    "X_train_stack = utils.clean_questions(utils.stack_questions(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:55:20.318792Z",
     "start_time": "2018-11-28T23:54:48.606183Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(X_train_stack)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(X_train_stack)\n",
    "data = pad_sequences(sequences, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:55:20.328275Z",
     "start_time": "2018-11-28T23:55:20.321523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(606398, 100)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:55:20.352546Z",
     "start_time": "2018-11-28T23:55:20.332045Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7212"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:,0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limited the questions to 100 words. \n",
    "\n",
    "**Note to self**\n",
    "* Lemmatization transformation could help\n",
    "* Not removing numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:59:00.965247Z",
     "start_time": "2018-11-28T23:55:20.356138Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocabulary_size, 300))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "#     print(word, index, end='\\r')\n",
    "    if index > vocabulary_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = utils.nlp(word).vector\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build out legs of the siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:12:16.352145Z",
     "start_time": "2018-11-29T00:12:16.302955Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Embedding' object has no attribute 'add'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-a75608eef107>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# conv_lstm_net = Sequential()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mconv_lstm_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mconv_lstm_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mconv_lstm_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mconv_lstm_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Embedding' object has no attribute 'add'"
     ]
    }
   ],
   "source": [
    "input_shape = (100,)\n",
    "input_l = Input(input_shape)\n",
    "input_r = Input(input_shape)\n",
    "\n",
    "# conv_lstm_net = Sequential()\n",
    "conv_lstm_net = Embedding(vocabulary_size, 300, input_length=100, weights=[embedding_matrix], trainable=False)\n",
    "conv_lstm_net.add(Dropout(0.2))\n",
    "conv_lstm_net.add(Conv1D(64, 5, activation='relu'))\n",
    "conv_lstm_net.add(MaxPooling1D(pool_size=4))\n",
    "conv_lstm_net.add(LSTM(300))\n",
    "conv_lstm_net.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "encoded_l = conv_lstm_net(input_l)\n",
    "encoded_r = conv_lstm_net(input_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:07:09.765140Z",
     "start_time": "2018-11-29T00:07:09.680279Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/project5/lib/python3.6/site-packages/keras/engine/network.py:180: UserWarning: Model inputs must come from `keras.layers.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to your model was not an Input tensor, it was generated by layer sequential_1.\n",
      "Note that input tensors are instantiated via `tensor = keras.layers.Input(shape)`.\n",
      "The tensor that caused the issue was: sequential_1/lstm_1/TensorArrayReadV3:0\n",
      "  str(x.name))\n",
      "/anaconda3/envs/project5/lib/python3.6/site-packages/keras/engine/network.py:180: UserWarning: Model inputs must come from `keras.layers.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to your model was not an Input tensor, it was generated by layer sequential_1.\n",
      "Note that input tensors are instantiated via `tensor = keras.layers.Input(shape)`.\n",
      "The tensor that caused the issue was: sequential_1_1/lstm_1/TensorArrayReadV3:0\n",
      "  str(x.name))\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-080868d1f3e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# both = concatenate([encoded_l,encoded_r], mode = L1_distance, output_shape=lambda x: x[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# prediction = Dense(1,activation='sigmoid')(both)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msiamese_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoded_l\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoded_r\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mSVG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msiamese_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'svg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/project5/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/project5/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/project5/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;31m# It's supposed to be an input layer, so only one node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;31m# and one tensor output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mnode_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtensor_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# L1_distance = lambda x: K.abs(x[0]-x[1])\n",
    "# siamese_net = Sequential()\n",
    "# siamese_net.add(Concatenate([encoded_l, encoded_r]))\n",
    "# siamese_net.add(Dense(1,activation='sigmoid'))\n",
    "# both = merge()\n",
    "# both = concatenate([encoded_l,encoded_r], mode = L1_distance, output_shape=lambda x: x[0])\n",
    "# prediction = Dense(1,activation='sigmoid')(both)\n",
    "siamese_net = Model(inputs=[encoded_l,encoded_r],outputs=prediction)\n",
    "SVG(model_to_dot(siamese_net, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:15:21.951772Z",
     "start_time": "2018-11-29T00:15:19.924658Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating word embedding layer\n",
    "embedding_layer = Embedding(vocabulary_size, 300, input_length=100, \n",
    "                                     weights=[embedding_matrix], trainable=False)\n",
    "\n",
    "# Creating LSTM Encoder\n",
    "# Bidirectional(LSTM(self.number_lstm_units, dropout=self.rate_drop_lstm, recurrent_dropout=self.rate_drop_lstm))\n",
    "lstm_layer = LSTM(300)\n",
    "\n",
    "# Creating LSTM Encoder layer for First Sentence\n",
    "sequence_1_input = Input(shape=(100,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "# Creating LSTM Encoder layer for Second Sentence\n",
    "sequence_2_input = Input(shape=(100,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "x2 = lstm_layer(embedded_sequences_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:16:12.624956Z",
     "start_time": "2018-11-29T00:16:12.372819Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import concatenate\n",
    "# Merging two LSTM encodes vectors from sentences to\n",
    "# pass it to dense layer applying dropout and batch normalisation\n",
    "merged = concatenate([x1, x2])\n",
    "# merged = BatchNormalization()(merged)\n",
    "# merged = Dropout(self.rate_drop_dense)(merged)\n",
    "# merged = Dense(self.number_dense_units, activation=self.activation_function)(merged)\n",
    "# merged = BatchNormalization()(merged)\n",
    "# merged = Dropout(self.rate_drop_dense)(merged)\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "# SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 300)     6000000     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 300)          721200      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 600)          0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            601         concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 6,721,801\n",
      "Trainable params: 721,801\n",
      "Non-trainable params: 6,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the batch to pass into the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:25:57.617365Z",
     "start_time": "2018-11-29T00:25:57.429291Z"
    }
   },
   "outputs": [],
   "source": [
    "odd_idx = [i for i in range(data.shape[0]) if i % 2 == 1]\n",
    "even_idx = [i for i in range(data.shape[0]) if i % 2 == 0]\n",
    "\n",
    "train_data_x1 = data[odd_idx[:10000]]\n",
    "train_data_x2 = data[even_idx[:10000]]\n",
    "train_labels = y_train[:10000]\n",
    "\n",
    "val_data_x1 = data[odd_idx[10000:11000]]\n",
    "val_data_x2 = data[even_idx[10000:11000]]\n",
    "val_labels = y_train[10000:11000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-29T00:26:39.525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 86s 9ms/step - loss: 0.6226 - acc: 0.6648 - val_loss: 0.6235 - val_acc: 0.6700\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 74s 7ms/step - loss: 0.6108 - acc: 0.6914 - val_loss: 0.6161 - val_acc: 0.6600\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 70s 7ms/step - loss: 0.5608 - acc: 0.7164 - val_loss: 0.5872 - val_acc: 0.6820\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 72s 7ms/step - loss: 0.4878 - acc: 0.7668 - val_loss: 0.5715 - val_acc: 0.7070\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 67s 7ms/step - loss: 0.4059 - acc: 0.8188 - val_loss: 0.6005 - val_acc: 0.7030\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 0.2948 - acc: 0.8823 - val_loss: 0.7115 - val_acc: 0.6740\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 0.1691 - acc: 0.9439 - val_loss: 0.8075 - val_acc: 0.6840\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: 0.0809 - acc: 0.9779 - val_loss: 0.9791 - val_acc: 0.6900\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: 0.0408 - acc: 0.9911 - val_loss: 1.0932 - val_acc: 0.7020\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 69s 7ms/step - loss: 0.0211 - acc: 0.9966 - val_loss: 1.2171 - val_acc: 0.6990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6850338080>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([train_data_x1, train_data_x2], train_labels,\n",
    "                  validation_data=([val_data_x1, val_data_x2], val_labels),\n",
    "                  epochs=10, batch_size=64, shuffle=True)\n",
    "# ,\n",
    "#                   callbacks=[early_stopping, model_checkpoint, tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfit like crazy, so need to add drop out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
