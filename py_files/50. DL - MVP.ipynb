{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MVP Siamese LSTM Net\n",
    "\n",
    "This is a baseline siamese LSTM net. The purpose is to build out the architecture, and see if the net can get as good as validation score as the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:05:40.872824Z",
     "start_time": "2018-11-29T00:05:40.835246Z"
    }
   },
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Input, Add, concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "# plotting\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:54:40.060662Z",
     "start_time": "2018-11-28T23:54:39.617510Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = utils.load('X_train')\n",
    "y_train = utils.load('y_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Encode vocabulary\n",
    "\n",
    "1. Limit the vocab to 20,000 words.\n",
    "2. Clean questions only and do not lemmatize.\n",
    "3. Limit the question length to 100 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:54:48.602373Z",
     "start_time": "2018-11-28T23:54:40.063608Z"
    }
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 20000\n",
    "X_train_stack = utils.clean_questions(utils.stack_questions(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:55:20.318792Z",
     "start_time": "2018-11-28T23:54:48.606183Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(X_train_stack)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(X_train_stack)\n",
    "data = pad_sequences(sequences, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:55:20.328275Z",
     "start_time": "2018-11-28T23:55:20.321523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(606398, 100)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:55:20.352546Z",
     "start_time": "2018-11-28T23:55:20.332045Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7212"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:,0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Matrix\n",
    "\n",
    "1. Calculates the embedding matrix utilizing spaCy `en_core_web_lg` word vectors.\n",
    "  * https://spacy.io/models/en#en_core_web_lg\n",
    "  * GloVe vectors trained on Common Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:59:00.965247Z",
     "start_time": "2018-11-28T23:55:20.356138Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    embedding_matrix = utils.load('embedding_matrix')\n",
    "except:\n",
    "    # create a weight matrix for words in training docs\n",
    "    embedding_matrix = np.zeros((vocabulary_size, 300))\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "    #     print(word, index, end='\\r')\n",
    "        if index > vocabulary_size - 1:\n",
    "            break\n",
    "        else:\n",
    "            embedding_vector = utils.nlp(word).vector\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[index] = embedding_vector\n",
    "    #     break\n",
    "\n",
    "    utils.save(embedding_matrix, 'embedding_matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the batch to pass into the network\n",
    "\n",
    "Create arrays to split the stacked data into question 1 set and question 2 set for each pair.\n",
    "\n",
    "**Need to cleanup this cell in the next model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:25:57.617365Z",
     "start_time": "2018-11-29T00:25:57.429291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train neg class: 0.63\n",
      "Val neg class: 0.61\n"
     ]
    }
   ],
   "source": [
    "odd_idx = [i for i in range(data.shape[0]) if i % 2 == 1]\n",
    "even_idx = [i for i in range(data.shape[0]) if i % 2 == 0]\n",
    "\n",
    "## TESTING - cleanup in next notebook \n",
    "# train_data_x1 = data[odd_idx[:10000]]\n",
    "# train_data_x2 = data[even_idx[:10000]]\n",
    "# train_labels = y_train[:10000]\n",
    "\n",
    "# val_data_x1 = data[odd_idx[10000:11000]]\n",
    "# val_data_x2 = data[even_idx[10000:11000]]\n",
    "# val_labels = y_train[10000:11000]\n",
    "\n",
    "print(f'Train neg class: {len(train_labels[train_labels == 0]) / len(train_labels):.2}')\n",
    "print(f'Val neg class: {len(val_labels[val_labels == 0]) / len(val_labels):.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build out legs of the siamese network\n",
    "\n",
    "The architecure is the following,\n",
    "\n",
    "0. Input - (100,) word tensor\n",
    "1. Embedding Layer - outputs (300,) **not trainable**\n",
    "2. LSTM - default outputs (300,)\n",
    "3. Concatenate the two nets outputs (600,)\n",
    "4. Dropout - 20%\n",
    "5. Dense - outputs (100,), activation `tanh` -- somewhat random decision\n",
    "6. Dropout - 20%\n",
    "7. Dense - outputs (1,), activation `sigmoid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:15:21.951772Z",
     "start_time": "2018-11-29T00:15:19.924658Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating word embedding layer\n",
    "embedding_layer = Embedding(vocabulary_size, 300, input_length=100, \n",
    "                                     weights=[embedding_matrix], trainable=False)\n",
    "\n",
    "# Creating LSTM Encoder\n",
    "# Bidirectional(LSTM(self.number_lstm_units, dropout=self.rate_drop_lstm, recurrent_dropout=self.rate_drop_lstm))\n",
    "lstm_layer = LSTM(300)\n",
    "\n",
    "# Creating LSTM Encoder layer for First Sentence\n",
    "sequence_1_input = Input(shape=(100,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "# Creating LSTM Encoder layer for Second Sentence\n",
    "sequence_2_input = Input(shape=(100,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "x2 = lstm_layer(embedded_sequences_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:16:12.624956Z",
     "start_time": "2018-11-29T00:16:12.372819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 300)     6000000     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 300)          721200      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 600)          0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 600)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          60100       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            101         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,781,401\n",
      "Trainable params: 781,401\n",
      "Non-trainable params: 6,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Merging two LSTM encodes vectors from sentences to\n",
    "# pass it to dense layer applying dropout and batch normalisation\n",
    "merged = concatenate([x1, x2])\n",
    "# merged = BatchNormalization()(merged)\n",
    "merged = Dropout(.2)(merged)\n",
    "merged = Dense(100, activation='tanh')(merged)\n",
    "# merged = BatchNormalization()(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "# SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/keras_models/mvp_{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(filepath=file_path)\n",
    "\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='../data/tensorboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-29T00:26:39.525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203143 samples, validate on 100056 samples\n",
      "Epoch 1/10\n",
      "203143/203143 [==============================] - 1102s 5ms/step - loss: 0.5499 - acc: 0.7221 - val_loss: 0.5066 - val_acc: 0.7530\n",
      "Epoch 2/10\n",
      "203143/203143 [==============================] - 1034s 5ms/step - loss: 0.4834 - acc: 0.7684 - val_loss: 0.4723 - val_acc: 0.7756\n",
      "Epoch 3/10\n",
      "203143/203143 [==============================] - 1119s 6ms/step - loss: 0.4418 - acc: 0.7952 - val_loss: 0.4578 - val_acc: 0.7838\n",
      "Epoch 4/10\n",
      "203143/203143 [==============================] - 1255s 6ms/step - loss: 0.4591 - acc: 0.7816 - val_loss: 0.4670 - val_acc: 0.7790\n",
      "Epoch 5/10\n",
      "203143/203143 [==============================] - 1098s 5ms/step - loss: 0.4111 - acc: 0.8130 - val_loss: 0.4591 - val_acc: 0.7852\n",
      "Epoch 6/10\n",
      " 84864/203143 [===========>..................] - ETA: 8:05 - loss: 0.3549 - acc: 0.8450"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203143/203143 [==============================] - 1171s 6ms/step - loss: 0.3303 - acc: 0.8579 - val_loss: 0.4551 - val_acc: 0.7978\n",
      "Epoch 8/10\n",
      "203143/203143 [==============================] - 1441s 7ms/step - loss: 0.3001 - acc: 0.8733 - val_loss: 0.4715 - val_acc: 0.7984\n",
      "Epoch 9/10\n",
      "203143/203143 [==============================] - 1328s 7ms/step - loss: 0.2765 - acc: 0.8849 - val_loss: 0.4898 - val_acc: 0.7939\n",
      "Epoch 10/10\n",
      "203143/203143 [==============================] - 1670s 8ms/step - loss: 0.3228 - acc: 0.8581 - val_loss: 0.4729 - val_acc: 0.7835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8db42e2c88>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([data[odd_idx], data[even_idx]], y_train, validation_split=0.33,\n",
    "                  epochs=10, batch_size=64, shuffle=True,\n",
    "                  callbacks=[model_checkpoint, tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The best average validation accuracy from the classification models is 0.783667, so very similar validation accuracy.\n",
    "\n",
    "### Next Steps\n",
    "Implement one or many of the ideas below.\n",
    "\n",
    "Future Ideas:\n",
    "* Explore LSTM settings\n",
    "* Dropout rates\n",
    "* Adding or removing the dense layers\n",
    "* Add BatchNormalization - theoritically speeds up training\n",
    "  * https://arxiv.org/pdf/1502.03167.pdf\n",
    "* Add EarlyStopping\n",
    "\n",
    "Change the validation scoring AUC, since the majority class represents 63% of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Code\n",
    "\n",
    "Example below is how to calculate AUC at the end of each epoch on the validation data. **Add this to the next model**.\n",
    "\n",
    "https://gist.github.com/smly/d29d079100f8d81b905e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An example to check the AUC score on a validation set for each 10 epochs.\n",
    "I hope it will be helpful for optimizing number of epochs.\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "# -*- coding: utf-8 -*-\n",
    "import logging\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "class IntervalEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=10):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict_proba(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            logging.info(\"interval evaluation - epoch: {:d} - score: {:.6f}\".format(epoch, score))\n",
    "            \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
