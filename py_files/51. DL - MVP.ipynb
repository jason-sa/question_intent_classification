{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MVP Siamese LSTM Net\n",
    "\n",
    "This is a baseline siamese LSTM net. The purpose is to build out the architecture, and see if the net can get as good as validation score as the classifiers.\n",
    "\n",
    "Ideas Implemented:\n",
    "* Add BatchNormalization - theoritically speeds up training\n",
    "  * https://arxiv.org/pdf/1502.03167.pdf\n",
    "* Add EarlyStopping\n",
    "* Add validation AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:05:40.872824Z",
     "start_time": "2018-11-29T00:05:40.835246Z"
    }
   },
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Input, Add, concatenate, BatchNormalization\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# plotting\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:54:40.060662Z",
     "start_time": "2018-11-28T23:54:39.617510Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = utils.load('X_train')\n",
    "y_train = utils.load('y_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Encode vocabulary\n",
    "\n",
    "1. Limit the vocab to 20,000 words.\n",
    "2. Clean questions only and do not lemmatize.\n",
    "3. Limit the question length to 100 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:54:48.602373Z",
     "start_time": "2018-11-28T23:54:40.063608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(606398, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7212"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = 20000\n",
    "max_q_len = 100\n",
    "\n",
    "X_train_stack = utils.clean_questions(utils.stack_questions(X_train))\n",
    "\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(X_train_stack)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(X_train_stack)\n",
    "data = pad_sequences(sequences, maxlen=max_q_len)\n",
    "\n",
    "print(data.shape)\n",
    "data[:,0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Matrix\n",
    "\n",
    "1. Calculates the embedding matrix utilizing spaCy `en_core_web_lg` word vectors.\n",
    "  * https://spacy.io/models/en#en_core_web_lg\n",
    "  * GloVe vectors trained on Common Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T23:59:00.965247Z",
     "start_time": "2018-11-28T23:55:20.356138Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    embedding_matrix = utils.load('embedding_matrix')\n",
    "except:\n",
    "    # create a weight matrix for words in training docs\n",
    "    embedding_matrix = np.zeros((vocabulary_size, 300))\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "    #     print(word, index, end='\\r')\n",
    "        if index > vocabulary_size - 1:\n",
    "            break\n",
    "        else:\n",
    "            embedding_vector = utils.nlp(word).vector\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[index] = embedding_vector\n",
    "    #     break\n",
    "\n",
    "    utils.save(embedding_matrix, 'embedding_matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the batch to pass into the network\n",
    "\n",
    "Create arrays to split the stacked data into question 1 set and question 2 set for each pair.\n",
    "\n",
    "**Need to cleanup this cell in the next model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:25:57.617365Z",
     "start_time": "2018-11-29T00:25:57.429291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train major class: 0.63\n",
      "Val major class: 0.63\n"
     ]
    }
   ],
   "source": [
    "# cooncatenate the two questions\n",
    "odd_idx = [i for i in range(data.shape[0]) if i % 2 == 1]\n",
    "even_idx = [i for i in range(data.shape[0]) if i % 2 == 0]\n",
    "\n",
    "data_1 = data[odd_idx]\n",
    "data_2 = data[odd_idx]\n",
    "\n",
    "# split the data set into a validation set\n",
    "data_train, data_val, label_train, label_val = train_test_split(np.hstack([data_1, data_2]), \n",
    "                                                                y_train, \n",
    "                                                                stratify=y_train, \n",
    "                                                                test_size = 0.33,\n",
    "                                                                random_state=42)\n",
    "\n",
    "# split the concatenation back into 2 data sets for the siamese network\n",
    "data_1_train = data_train[:, :max_q_len]\n",
    "data_2_train = data_train[:, max_q_len:]\n",
    "data_1_val = data_val[:, :max_q_len]\n",
    "data_2_val = data_val[:, max_q_len:]\n",
    "\n",
    "print(f'Train major class: {len(label_train[label_train == 0]) / len(label_train):.2}')\n",
    "print(f'Val major class: {len(label_val[label_val == 0]) / len(label_val):.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate AUC after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An example to check the AUC score on a validation set for each 10 epochs.\n",
    "I hope it will be helpful for optimizing number of epochs.\n",
    "\"\"\"\n",
    "\n",
    "class IntervalEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=10):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            logging.info(\"interval evaluation - epoch: {:d} - score: {:.6f}\".format(epoch, score))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build out legs of the siamese network\n",
    "\n",
    "The architecure is the following,\n",
    "\n",
    "0. Input - (100,) word tensor\n",
    "1. Embedding Layer - outputs (300,) **not trainable**\n",
    "2. LSTM - default outputs (300,)\n",
    "3. Concatenate the two nets outputs (600,)\n",
    "4. BatchNormalization\n",
    "5. Dropout - 20%\n",
    "6. Dense - outputs (100,), activation `tanh` -- somewhat random decision\n",
    "7. BatchNormalization\n",
    "8. Dropout - 20%\n",
    "9. Dense - outputs (1,), activation `sigmoid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:15:21.951772Z",
     "start_time": "2018-11-29T00:15:19.924658Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating word embedding layer\n",
    "embedding_layer = Embedding(vocabulary_size, 300, input_length=100, \n",
    "                                     weights=[embedding_matrix], trainable=False)\n",
    "\n",
    "# Creating LSTM Encoder\n",
    "# Bidirectional(LSTM(self.number_lstm_units, dropout=self.rate_drop_lstm, recurrent_dropout=self.rate_drop_lstm))\n",
    "lstm_layer = LSTM(300)\n",
    "\n",
    "# Creating LSTM Encoder layer for First Sentence\n",
    "sequence_1_input = Input(shape=(100,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "# Creating LSTM Encoder layer for Second Sentence\n",
    "sequence_2_input = Input(shape=(100,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "x2 = lstm_layer(embedded_sequences_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T00:16:12.624956Z",
     "start_time": "2018-11-29T00:16:12.372819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 100, 300)     6000000     input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 300)          721200      embedding_2[0][0]                \n",
      "                                                                 embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 600)          0           lstm_2[0][0]                     \n",
      "                                                                 lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 600)          2400        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 600)          0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 100)          60100       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 100)          400         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 100)          0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            101         dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,784,201\n",
      "Trainable params: 782,801\n",
      "Non-trainable params: 6,001,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Merging two LSTM encodes vectors from sentences to\n",
    "# pass it to dense layer applying dropout and batch normalisation\n",
    "\n",
    "merged = concatenate([x1, x2])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(.2)(merged)\n",
    "merged = Dense(100, activation='tanh')(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "# SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "\n",
    "file_path = '../data/keras_models/mvp_batch_norm{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(filepath=file_path, save_best_only=True)\n",
    "\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='../data/tensorboard')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                               min_delta=0, \n",
    "                               patience=3, \n",
    "                               verbose=1, \n",
    "                               mode='auto', \n",
    "                               restore_best_weights=True)\n",
    "\n",
    "calc_auc = IntervalEvaluation(([data_1_val, data_2_val], label_val), interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-29T00:26:39.525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203143 samples, validate on 100056 samples\n",
      "Epoch 1/20\n",
      "203143/203143 [==============================] - 1087s 5ms/step - loss: 0.5447 - acc: 0.7258 - val_loss: 0.5413 - val_acc: 0.7250\n",
      "Epoch 2/20\n",
      "203143/203143 [==============================] - 1086s 5ms/step - loss: 0.5219 - acc: 0.7412 - val_loss: 0.5275 - val_acc: 0.7386\n",
      "Epoch 3/20\n",
      "203143/203143 [==============================] - 1086s 5ms/step - loss: 0.5026 - acc: 0.7546 - val_loss: 0.5203 - val_acc: 0.7448\n",
      "Epoch 4/20\n",
      "203143/203143 [==============================] - 1083s 5ms/step - loss: 0.4822 - acc: 0.7688 - val_loss: 0.5172 - val_acc: 0.7459\n",
      "Epoch 5/20\n",
      " 99392/203143 [=============>................] - ETA: 7:47 - loss: 0.4543 - acc: 0.7856"
     ]
    }
   ],
   "source": [
    "model.fit([data_1_train, data_2_train], label_train, \n",
    "          validation_data=([data_1_val, data_2_val], label_val),\n",
    "                  epochs=20, batch_size=64, shuffle=True,\n",
    "                  callbacks=[model_checkpoint, tensorboard, early_stopping, calc_auc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The best average validation accuracy from the classification models is 0.783667, so very similar validation accuracy.\n",
    "\n",
    "### Next Steps\n",
    "Implement one or many of the ideas below.\n",
    "\n",
    "Future Ideas:\n",
    "* Explore LSTM settings\n",
    "* Dropout rates\n",
    "* Adding or removing the dense layers\n",
    "* Add BatchNormalization - theoritically speeds up training\n",
    "  * https://arxiv.org/pdf/1502.03167.pdf\n",
    "* Add EarlyStopping\n",
    "\n",
    "Change the validation scoring AUC, since the majority class represents 63% of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Code\n",
    "\n",
    "Example below is how to calculate AUC at the end of each epoch on the validation data. **Add this to the next model**.\n",
    "\n",
    "https://gist.github.com/smly/d29d079100f8d81b905e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
