{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan of attack\n",
    "\n",
    "1. Use spacy to better tokeninze the text.\n",
    "2. Build an custom text cleaner step in the pipeline process.\n",
    "3. Build document vector conversion step.\n",
    "4. Build the pairing of the questions, or somehow build in parallel.\n",
    "5. Run XGBoost to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T02:57:41.372894Z",
     "start_time": "2018-11-25T02:57:25.639976Z"
    }
   },
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "from utils import save, load\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# text manipulation\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# modeling\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T01:07:03.607349Z",
     "start_time": "2018-11-25T01:07:02.414919Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = load('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to clean bad text, and tokenize the Quora questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T01:11:01.048959Z",
     "start_time": "2018-11-25T01:11:01.042644Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(question):\n",
    "    ''' Pre-processeor to clean the Quora questions\n",
    "    '''\n",
    "    # found 1 example of no space after a question mark, which causes issues with the tokenzier\n",
    "    for p in punctuations:\n",
    "        question = question.replace(p, ' ')\n",
    "    \n",
    "    return question\n",
    "\n",
    "def spacy_tokenizer(question):\n",
    "    ''' Tokenizer that lemmatizes and removes stop words and punctuation\n",
    "    '''\n",
    "    tokens = nlp(clean_text(question))\n",
    "    tokens = [tok.lemma_.lower().strip() for tok in tokens if tok.lemma_ != '-PRON-']\n",
    "    tokens = [tok for tok in tokens if tok not in punctuations and tok not in spacy_stopwords]     \n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T01:12:20.116733Z",
     "start_time": "2018-11-25T01:12:20.090278Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How can I direct message someone on Instagram from my computer?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = train_df.loc[:,'question1'].sample(1).values[0]\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T01:12:20.593435Z",
     "start_time": "2018-11-25T01:12:20.568171Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['direct', 'message', 'instagram', 'computer']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tokenizer(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline will be the following,\n",
    "\n",
    "1. Transform pairs into documents\n",
    "2. TF-IDF of the transformation, with the `spacy_tokenizer`\n",
    "3. NMF with 50 components\n",
    "4. Transform the document back into pairs\n",
    "5. XGBoost classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T03:02:33.255636Z",
     "start_time": "2018-11-25T03:02:33.247517Z"
    }
   },
   "outputs": [],
   "source": [
    "def stack_questions(df):\n",
    "    ''' Takes the pair of questions, and stacks them as individual documents to be processed.\n",
    "    \n",
    "    df: DataFrame \n",
    "    The data frame must have the 3 cols (id, question1, question2).\n",
    "    \n",
    "    return: DataFrame\n",
    "    Returns a data frame of documents (questions)\n",
    "    '''\n",
    "    X = df.loc[:, ['id', 'question1']]\n",
    "    df = df.drop(columns='question1')\n",
    "    df = df.rename(columns={'question2':'question1'})\n",
    "    \n",
    "    X = X.append(df.loc[:, ['id', 'question1']], sort=False)\n",
    "    X = X.sort_values('id').reset_index()\n",
    "    \n",
    "    return np.array(X['question1'])\n",
    "\n",
    "def unstack_questions(X):\n",
    "    ''' Takes X (n_question*2, 1) and transforms it to a (n_questions, 2) numpy array. \n",
    "    '''\n",
    "    odd_idx = [i for i in range(len(X)) if i % 2 == 1]\n",
    "    even_idx = [i for i in range(len(X)) if i % 2 == 0]\n",
    "    \n",
    "    return np.vstack([X[odd_idx], X[even_idx]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build a simple pipeline to confirm the questions can be transformed from pairs, to an array of documents, back to pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T03:03:49.795677Z",
     "start_time": "2018-11-25T03:03:48.914221Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['What is the step by step guide to invest in share market?',\n",
       "        'What is the step by step guide to invest in share market in india?'],\n",
       "       ['What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?',\n",
       "        'What is the story of Kohinoor (Koh-i-Noor) Diamond?'],\n",
       "       ['How can Internet speed be increased by hacking through DNS?',\n",
       "        'How can I increase the speed of my internet connection while using a VPN?'],\n",
       "       ['Find the remainder when [math]23^{24}[/math] is divided by 24,23?',\n",
       "        'Why am I mentally very lonely? How can I solve it?'],\n",
       "       ['Which fish would survive in salt water?',\n",
       "        'Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_pip = Pipeline(\n",
    "    [\n",
    "        ('stack', FunctionTransformer(stack_questions, validate=False)),\n",
    "        ('unstack', FunctionTransformer(unstack_questions, validate=False))\n",
    "    ]\n",
    ")\n",
    "\n",
    "simple_pip.transform(train_df)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:project5]",
   "language": "python",
   "name": "conda-env-project5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
