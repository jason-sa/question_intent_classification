{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced featrue engineer model\n",
    "\n",
    "This model will add engineered features for the original question, in addition to the lemmatized question.\n",
    "\n",
    "Engineered two different types of features,\n",
    "\n",
    "1. n_gram similarity between each pair of questions\n",
    "2. min/max/avg distance between words in a single question. Currently using the following metrics,\n",
    "  * euclidean\n",
    "  * cosine\n",
    "  * city block or manhattan\n",
    "  \n",
    "**Pipeline**\n",
    "1. Stack questions\n",
    "2. Clean questions - now lower cases all words to better lemmatize proper nouns\n",
    "3. UNION\n",
    "    1. n_gram similarity\n",
    "    2. min/max/avg distance\n",
    "4. Lemmatize questions\n",
    "5. UNION\n",
    "    1. n_gram similarity\n",
    "    2. min/max/avg distances\n",
    "6. UNION together both sets of features\n",
    "7. Avearge Ensemble\n",
    "    1. Random Forrest\n",
    "    2. XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T04:57:04.646711Z",
     "start_time": "2018-11-28T04:56:47.834474Z"
    }
   },
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# modeling\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T04:57:05.108061Z",
     "start_time": "2018-11-28T04:57:04.649533Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = utils.load('X_train')\n",
    "y_train = utils.load('y_train')\n",
    "MEM_PATH = '../data/transform_memory'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_pipe = Pipeline(\n",
    "    [\n",
    "        ('stack', FunctionTransformer(utils.stack_questions, validate=False)),\n",
    "        ('clean', FunctionTransformer(utils.clean_questions, validate=False))\n",
    "    ],\n",
    "    memory = MEM_PATH\n",
    ")\n",
    "\n",
    "lemma_pipe = Pipeline(\n",
    "    [\n",
    "        ('lemma', FunctionTransformer(utils.apply_lemma, validate=False))\n",
    "    ],\n",
    "    memory = MEM_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cos similarity of TF-IDF vector plus NMF topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_pipe = Pipeline(\n",
    "    [\n",
    "        ('nmf', NMF(n_components=5)),\n",
    "        ('unstack', FunctionTransformer(utils.unstack_questions, validate=True))\n",
    "    ]\n",
    ")\n",
    "\n",
    "cos_pipe = Pipeline(\n",
    "    [\n",
    "        ('cos', FunctionTransformer(utils.calc_cos_sim_stack, validate=False))\n",
    "    ]\n",
    ")\n",
    "\n",
    "nmf_cos_pipe = Pipeline(\n",
    "    [\n",
    "        ('clean', clean_pipe),\n",
    "        ('lemma', lemma_pipe),\n",
    "        ('tf', TfidfVectorizer()),\n",
    "        ('feats', FeatureUnion(\n",
    "            [\n",
    "                ('nmf_pipe', nmf_pipe),\n",
    "                ('cos_pipe', cos_pipe)\n",
    "            ]\n",
    "        )),\n",
    "        ('xgb', XGBClassifier(n_estimators=500, n_jobs=-1, random_state=42))\n",
    "    ]\n",
    ")\n",
    "# X_transform = pipe.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T04:57:05.128186Z",
     "start_time": "2018-11-28T04:57:05.110672Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature engineering pipes\n",
    "single_question_pipe = Pipeline(\n",
    "    [\n",
    "        ('dist', FunctionTransformer(utils.add_min_max_avg_distance_features, validate=False)),\n",
    "        ('unstack', FunctionTransformer(utils.unstack_questions, validate=False))\n",
    "    ]\n",
    ")\n",
    "\n",
    "pair_question_pipe = Pipeline(\n",
    "    [\n",
    "        ('ngram_sim', FunctionTransformer(utils.calc_ngram_similarity, kw_args={'n_grams':[1, 2, 3]}, validate=False))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# clean text pipe\n",
    "clean_text_pipe = Pipeline(\n",
    "    [\n",
    "        ('clean', clean_pipe),\n",
    "        ('feats', FeatureUnion(\n",
    "            [\n",
    "                ('pair', pair_question_pipe),\n",
    "                ('single', single_question_pipe)\n",
    "            ]\n",
    "        ))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# lemma pipe\n",
    "lemma_text_pipe = Pipeline(\n",
    "    [\n",
    "        ('clean', clean_pipe),\n",
    "        ('lemma', lemma_pipe),\n",
    "        ('feats', FeatureUnion(\n",
    "            [\n",
    "                ('pair', pair_question_pipe),\n",
    "                ('single', single_question_pipe)\n",
    "            ]\n",
    "        ))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T04:57:05.128186Z",
     "start_time": "2018-11-28T04:57:05.110672Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "XGB_pipe = Pipeline(\n",
    "    [\n",
    "        ('feats', FeatureUnion(\n",
    "            [\n",
    "                ('clean_features', clean_text_pipe),\n",
    "                ('lemma_pipe', lemma_text_pipe)\n",
    "            ]\n",
    "        )),\n",
    "        ('xgb', XGBClassifier(n_estimators=500, n_jobs=-1, random_state=42))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_pipe = Pipeline(\n",
    "    [\n",
    "        ('feats', FeatureUnion(\n",
    "            [\n",
    "                ('clean_features', clean_text_pipe),\n",
    "                ('lemma_pipe', lemma_text_pipe)\n",
    "            ]\n",
    "        )),\n",
    "        ('rf', RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T04:57:05.128186Z",
     "start_time": "2018-11-28T04:57:05.110672Z"
    }
   },
   "outputs": [],
   "source": [
    "# weighting based on individual AUC\n",
    "## cos_sim = 0.799173\n",
    "## xgboost = 0.846923\n",
    "## rf = 0.868202\n",
    "\n",
    "total = 0.799173 + 0.846923 + 0.868202\n",
    "cos_sim_weight = 0.799173 / total\n",
    "xgb_weight = 0.846923 / total\n",
    "rf_weight = 0.868202 / total\n",
    "\n",
    "weights = [cos_sim_weight, xgb_weight, rf_weight]\n",
    "\n",
    "# nmf_cos = utils.load('cos_sim_tfidf_model')\n",
    "# xgb = utils.load('xgb_feat_eng_model')\n",
    "# rf = utils.load('rf_feat_eng_model')\n",
    "\n",
    "estimators = [('cos_sim', nmf_cos_pipe), ('xgb', XGB_pipe), ('rf', RF_pipe)]\n",
    "vc = VotingClassifier(estimators, voting='soft', n_jobs=1, weights=weights) \n",
    "## don't think this works with what I want to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T04:57:09.161680Z",
     "start_time": "2018-11-28T04:57:05.134427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[CV]  ................................................................\n",
      "[CV]  , accuracy=0.7366526694661067, precision=0.6607929515418502, recall=0.6033789219629928, f1=0.6307821698906645, roc_auc=0.8161449694568877, neg_log_loss=-0.4969755618973226, total= 4.5min\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 11.0min remaining:    0.0s\n",
      "[CV]  ................................................................\n",
      "[CV]  , accuracy=0.7314731473147315, precision=0.6494401378122309, recall=0.607085346215781, f1=0.6275488972118186, roc_auc=0.8166788729552542, neg_log_loss=-0.49579440384491175, total= 4.5min\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 21.9min remaining:    0.0s\n",
      "[CV]  ................................................................\n"
     ]
    }
   ],
   "source": [
    "# X_transform = pre_process_pipe.transform(X_train)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3, random_state=42)\n",
    "cv = cross_validate(vc, \n",
    "               X_train[:10000], \n",
    "               y_train[:10000], \n",
    "               cv=skf, \n",
    "               n_jobs=1, \n",
    "               scoring=('accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'neg_log_loss'),\n",
    "               verbose=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = utils.load('results')\n",
    "\n",
    "results_df = results_df.drop(index='ensemble_rf_xgb_cos_sim', errors='ignore')\n",
    "results_df = results_df.append(utils.log_scores(cv, 'ensemble_rf_xgb_cos_sim'))\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save(results_df, 'results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Wow! The feature engineering shows a significant jump in AUC from 0.8 to 0.82."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_we...ate=42, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=True, subsample=1))],\n",
       "         flatten_transform=None, n_jobs=None, voting='soft', weights=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc = VotingClassifier([('rf', rf), ('xgb', xgb)], voting='soft')\n",
    "vc.fit(X_transform, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gt</th>\n",
       "      <th>prob</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.271811</td>\n",
       "      <td>-0.271811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.331749</td>\n",
       "      <td>-0.331749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.158016</td>\n",
       "      <td>-0.158016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>-0.000237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.034689</td>\n",
       "      <td>-0.034689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gt      prob      diff\n",
       "0   0  0.271811 -0.271811\n",
       "1   0  0.331749 -0.331749\n",
       "2   0  0.158016 -0.158016\n",
       "3   0  0.000237 -0.000237\n",
       "4   0  0.034689 -0.034689"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_probs = vc.predict_proba(X_transform)[:, 1]\n",
    "class_errors_df = utils.ground_truth_analysis(y_train, y_probs)\n",
    "class_errors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_pipe = Pipeline(\n",
    "    [\n",
    "        ('stack', FunctionTransformer(utils.stack_questions, validate=False)),\n",
    "        ('clean', FunctionTransformer(utils.clean_questions, validate=False)),\n",
    "        ('lemma', FunctionTransformer(utils.apply_lemma, validate=False)),\n",
    "    ]\n",
    ")\n",
    "X_train_lemma = lemma_pipe.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top false negative errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_idx = class_errors_df.sort_values('diff', ascending = False).head().index\n",
    "for idx in fn_idx:\n",
    "    print('Prob:', y_probs[idx])\n",
    "    print()\n",
    "    print(X_train.iloc[idx].question1)\n",
    "    print(X_train.iloc[idx].question2)\n",
    "    print()\n",
    "    print('Lemma--------')\n",
    "    print()\n",
    "    print(X_train_lemma[idx*2])\n",
    "    print(X_train_lemma[idx*2+1])\n",
    "    print()\n",
    "    print('Feature Space------')\n",
    "    print(X_transform[idx])\n",
    "    print('-------------------------------------------')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top false positive errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_idx = class_errors_df.sort_values('diff').head().index\n",
    "for idx in fn_idx:\n",
    "    print('Prob:', y_probs[idx])\n",
    "    print()\n",
    "    print(X_train.iloc[idx].question1)\n",
    "    print(X_train.iloc[idx].question2)\n",
    "    print()\n",
    "    print('Lemma--------')\n",
    "    print()\n",
    "    print(X_train_lemma[idx*2])\n",
    "    print(X_train_lemma[idx*2+1])\n",
    "    print()\n",
    "    print('Feature Space------')\n",
    "    print(X_transform[idx])\n",
    "    print('-------------------------------------------')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. The false negative errors appear to be related to numbers. \n",
    "  * Either stop removing numebers in the clean text proces, or create another set of features.\n",
    "2. The false positives seem very tricky. Found the Facebook InferSent model which embeds sentences to 4096 dimension.\n",
    "  * Could see if the distances between some of the false positive examples is different in this space.\n",
    "3. Could look at alignments of the two questions. (I think nltk)\n",
    "4. Add different word embeddings to get a different nuance, or even use the vector only data set from Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_cos_proba = nmf_cos.predict_proba(X_nmf_cos_feat)[:, 1]\n",
    "xgb_proba = xgb.predict_proba(X_feat_eng_transform)[:, 1]\n",
    "rf_proba = rf.predict_proba(X_feat_eng_transform)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39567994, 0.34479889, 0.17470645, 0.00103656, 0.07662025,\n",
       "       0.45168266, 0.29587881, 0.14138014, 0.0013542 , 0.68504003])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_cos_proba = nmf_cos_proba * cos_sim_weight\n",
    "xgb_proba = xgb_proba * xgb_weight\n",
    "rf_proba = rf_proba * rf_weight\n",
    "ensemble_proba = nmf_cos_proba + xgb_proba + rf_proba\n",
    "ensemble_proba[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble: 0.9846300420170292\n",
      "Nmf_cos: 0.8056886449194024\n",
      "XGB: 0.8540229521257536\n",
      "RF: 0.9999764283543164\n"
     ]
    }
   ],
   "source": [
    "print(f'Ensemble: {metrics.roc_auc_score(y_train, ensemble_proba)}')\n",
    "print(f'Nmf_cos: {metrics.roc_auc_score(y_train, nmf_cos_proba)}')\n",
    "print(f'XGB: {metrics.roc_auc_score(y_train, xgb_proba)}')\n",
    "print(f'RF: {metrics.roc_auc_score(y_train, rf_proba)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble: 0.32102219836966456\n",
      "Nmf_cos: 0.5069449937264294\n",
      "XGB: 0.4495672382949828\n",
      "RF: 0.12308486086868513\n"
     ]
    }
   ],
   "source": [
    "print(f'Ensemble: {metrics.log_loss(y_train, ensemble_proba)}')\n",
    "print(f'Nmf_cos: {metrics.log_loss(y_train, nmf_cos_proba)}')\n",
    "print(f'XGB: {metrics.log_loss(y_train, xgb_proba)}')\n",
    "print(f'RF: {metrics.log_loss(y_train, rf_proba)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
