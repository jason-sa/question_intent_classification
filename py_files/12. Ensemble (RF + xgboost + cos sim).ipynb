{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced featrue engineer model\n",
    "\n",
    "This model will add engineered features for the original question, in addition to the lemmatized question.\n",
    "\n",
    "Engineered two different types of features,\n",
    "\n",
    "1. n_gram similarity between each pair of questions\n",
    "2. min/max/avg distance between words in a single question. Currently using the following metrics,\n",
    "  * euclidean\n",
    "  * cosine\n",
    "  * city block or manhattan\n",
    "  \n",
    "**Pipeline**\n",
    "1. Stack questions\n",
    "2. Clean questions - now lower cases all words to better lemmatize proper nouns\n",
    "3. UNION\n",
    "    1. n_gram similarity\n",
    "    2. min/max/avg distance\n",
    "4. Lemmatize questions\n",
    "5. UNION\n",
    "    1. n_gram similarity\n",
    "    2. min/max/avg distances\n",
    "6. UNION together both sets of features\n",
    "7. Avearge Ensemble\n",
    "    1. Random Forrest\n",
    "    2. XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T04:57:04.646711Z",
     "start_time": "2018-11-28T04:56:47.834474Z"
    }
   },
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# modeling\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T04:57:05.108061Z",
     "start_time": "2018-11-28T04:57:04.649533Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = utils.load('X_train')\n",
    "y_train = utils.load('y_train')\n",
    "MEM_PATH = '../data/transform_memory'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_pipe = Pipeline(\n",
    "    [\n",
    "        ('stack', FunctionTransformer(utils.stack_questions, validate=False)),\n",
    "        ('clean', FunctionTransformer(utils.clean_questions, validate=False))\n",
    "    ],\n",
    "    memory = MEM_PATH\n",
    ")\n",
    "\n",
    "lemma_pipe = Pipeline(\n",
    "    [\n",
    "        ('lemma', FunctionTransformer(utils.apply_lemma, validate=False))\n",
    "    ],\n",
    "    memory = MEM_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cos similarity of TF-IDF vector plus NMF topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_pipe = Pipeline(\n",
    "    [\n",
    "        ('nmf', NMF(n_components=5)),\n",
    "        ('unstack', FunctionTransformer(utils.unstack_questions, validate=True))\n",
    "    ]\n",
    ")\n",
    "\n",
    "cos_pipe = Pipeline(\n",
    "    [\n",
    "        ('cos', FunctionTransformer(utils.calc_cos_sim_stack, validate=False))\n",
    "    ]\n",
    ")\n",
    "\n",
    "nmf_cos_pipe = Pipeline(\n",
    "    [\n",
    "        ('clean', clean_pipe),\n",
    "        ('lemma', lemma_pipe),\n",
    "        ('tf', TfidfVectorizer()),\n",
    "        ('feats', FeatureUnion(\n",
    "            [\n",
    "                ('nmf_pipe', nmf_pipe),\n",
    "                ('cos_pipe', cos_pipe)\n",
    "            ]\n",
    "        )),\n",
    "        ('xgb', XGBClassifier(n_estimators=500, n_jobs=-1, random_state=42))\n",
    "    ]\n",
    ")\n",
    "# X_transform = pipe.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T04:57:05.128186Z",
     "start_time": "2018-11-28T04:57:05.110672Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature engineering pipes\n",
    "single_question_pipe = Pipeline(\n",
    "    [\n",
    "        ('dist', FunctionTransformer(utils.add_min_max_avg_distance_features, validate=False)),\n",
    "        ('unstack', FunctionTransformer(utils.unstack_questions, validate=False))\n",
    "    ]\n",
    ")\n",
    "\n",
    "pair_question_pipe = Pipeline(\n",
    "    [\n",
    "        ('ngram_sim', FunctionTransformer(utils.calc_ngram_similarity, kw_args={'n_grams':[1, 2, 3]}, validate=False))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# clean text pipe\n",
    "clean_text_pipe = Pipeline(\n",
    "    [\n",
    "        ('clean', clean_pipe),\n",
    "        ('feats', FeatureUnion(\n",
    "            [\n",
    "                ('pair', pair_question_pipe),\n",
    "                ('single', single_question_pipe)\n",
    "            ]\n",
    "        ))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# lemma pipe\n",
    "lemma_text_pipe = Pipeline(\n",
    "    [\n",
    "        ('clean', clean_pipe),\n",
    "        ('lemma', lemma_pipe),\n",
    "        ('feats', FeatureUnion(\n",
    "            [\n",
    "                ('pair', pair_question_pipe),\n",
    "                ('single', single_question_pipe)\n",
    "            ]\n",
    "        ))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T04:57:05.128186Z",
     "start_time": "2018-11-28T04:57:05.110672Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "XGB_pipe = Pipeline(\n",
    "    [\n",
    "        ('feats', FeatureUnion(\n",
    "            [\n",
    "                ('clean_features', clean_text_pipe),\n",
    "                ('lemma_pipe', lemma_text_pipe)\n",
    "            ]\n",
    "        )),\n",
    "        ('xgb', XGBClassifier(n_estimators=500, n_jobs=-1, random_state=42))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_pipe = Pipeline(\n",
    "    [\n",
    "        ('feats', FeatureUnion(\n",
    "            [\n",
    "                ('clean_features', clean_text_pipe),\n",
    "                ('lemma_pipe', lemma_text_pipe)\n",
    "            ]\n",
    "        )),\n",
    "        ('rf', RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T04:57:05.128186Z",
     "start_time": "2018-11-28T04:57:05.110672Z"
    }
   },
   "outputs": [],
   "source": [
    "# weighting based on individual AUC\n",
    "## cos_sim = 0.799173\n",
    "## xgboost = 0.846923\n",
    "## rf = 0.868202\n",
    "\n",
    "total = 0.799173 + 0.846923 + 0.868202\n",
    "cos_sim_weight = 0.799173 / total\n",
    "xgb_weight = 0.846923 / total\n",
    "rf_weight = 0.868202 / total\n",
    "\n",
    "weights = [cos_sim_weight, xgb_weight, rf_weight]\n",
    "\n",
    "# nmf_cos = utils.load('cos_sim_tfidf_model')\n",
    "# xgb = utils.load('xgb_feat_eng_model')\n",
    "# rf = utils.load('rf_feat_eng_model')\n",
    "\n",
    "estimators = [('cos_sim', nmf_cos_pipe), ('xgb', XGB_pipe), ('rf', RF_pipe)]\n",
    "vc = VotingClassifier(estimators, voting='soft', n_jobs=1, weights=weights) \n",
    "## don't think this works with what I want to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T04:57:09.161680Z",
     "start_time": "2018-11-28T04:57:05.134427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[CV]  ................................................................\n",
      "[CV]  , accuracy=0.7381523695260948, precision=0.6622807017543859, recall=0.6074014481094127, f1=0.6336550566512797, roc_auc=0.8162834782481563, neg_log_loss=-0.49677170945574356, total= 4.5min\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 10.9min remaining:    0.0s\n",
      "[CV]  ................................................................\n",
      "[CV]  , accuracy=0.72997299729973, precision=0.6474137931034483, recall=0.604669887278583, f1=0.6253122398001666, roc_auc=0.8163473393756387, neg_log_loss=-0.4961386748642456, total= 4.4min\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 21.8min remaining:    0.0s\n",
      "[CV]  ................................................................\n",
      "[CV]  , accuracy=0.7479747974797479, precision=0.6741767764298093, recall=0.6264090177133655, f1=0.649415692821369, roc_auc=0.8273306887658249, neg_log_loss=-0.4881980791914163, total= 4.4min\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 32.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 32.7min finished\n"
     ]
    }
   ],
   "source": [
    "# X_transform = pre_process_pipe.transform(X_train)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3, random_state=42)\n",
    "cv = cross_validate(vc, \n",
    "               X_train[:10000], \n",
    "               y_train[:10000], \n",
    "               cv=skf, \n",
    "               n_jobs=1, \n",
    "               scoring=('accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'neg_log_loss'),\n",
    "               verbose=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_accuracy</th>\n",
       "      <th>std_accuracy</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>std_precision</th>\n",
       "      <th>avg_recall</th>\n",
       "      <th>std_recall</th>\n",
       "      <th>avg_f1</th>\n",
       "      <th>std_f1</th>\n",
       "      <th>avg_auc</th>\n",
       "      <th>std_auc</th>\n",
       "      <th>avg_log_loss</th>\n",
       "      <th>std_log_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mvp (tf-idf, nmf(5), xgboost)</th>\n",
       "      <td>0.700345</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.661571</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.385736</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>0.487325</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.740593</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.568958</td>\n",
       "      <td>0.001288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mvp (+ lemma)</th>\n",
       "      <td>0.696787</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.649977</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.387424</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>0.485464</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>0.738037</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.572483</td>\n",
       "      <td>0.000815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cos_sim_model</th>\n",
       "      <td>0.710200</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.658748</td>\n",
       "      <td>0.002578</td>\n",
       "      <td>0.446336</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>0.532120</td>\n",
       "      <td>0.001306</td>\n",
       "      <td>0.746769</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.565250</td>\n",
       "      <td>0.000963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_eng_model</th>\n",
       "      <td>0.743614</td>\n",
       "      <td>0.002021</td>\n",
       "      <td>0.664102</td>\n",
       "      <td>0.003502</td>\n",
       "      <td>0.618400</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>0.640434</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>0.821070</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.489465</td>\n",
       "      <td>0.001141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_eng_model_lemma_fix</th>\n",
       "      <td>0.744356</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>0.664513</td>\n",
       "      <td>0.004333</td>\n",
       "      <td>0.621357</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.642201</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>0.822197</td>\n",
       "      <td>0.001710</td>\n",
       "      <td>0.488131</td>\n",
       "      <td>0.001342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensemble_rf_xgb</th>\n",
       "      <td>0.779000</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.697794</td>\n",
       "      <td>0.004357</td>\n",
       "      <td>0.708157</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.702935</td>\n",
       "      <td>0.003148</td>\n",
       "      <td>0.863334</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.441784</td>\n",
       "      <td>0.001107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cos_sim_tfidf_model</th>\n",
       "      <td>0.729511</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.661680</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.547188</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.599010</td>\n",
       "      <td>0.001703</td>\n",
       "      <td>0.800271</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>0.512085</td>\n",
       "      <td>0.001299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_eng_model_lemma_clean</th>\n",
       "      <td>0.763927</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.676166</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>0.692113</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.684044</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.846923</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>0.456929</td>\n",
       "      <td>0.001410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_feat_eng_model_lemma_clean</th>\n",
       "      <td>0.783667</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>0.708853</td>\n",
       "      <td>0.003681</td>\n",
       "      <td>0.702725</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>0.705774</td>\n",
       "      <td>0.002658</td>\n",
       "      <td>0.868202</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.436197</td>\n",
       "      <td>0.000640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensemble_rf_xgb_cos_sim</th>\n",
       "      <td>0.738700</td>\n",
       "      <td>0.007359</td>\n",
       "      <td>0.661290</td>\n",
       "      <td>0.010948</td>\n",
       "      <td>0.612827</td>\n",
       "      <td>0.009669</td>\n",
       "      <td>0.636128</td>\n",
       "      <td>0.009994</td>\n",
       "      <td>0.819987</td>\n",
       "      <td>0.005193</td>\n",
       "      <td>0.493703</td>\n",
       "      <td>0.003901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               avg_accuracy  std_accuracy  avg_precision  \\\n",
       "mvp (tf-idf, nmf(5), xgboost)      0.700345      0.000466       0.661571   \n",
       "mvp (+ lemma)                      0.696787      0.001055       0.649977   \n",
       "cos_sim_model                      0.710200      0.000830       0.658748   \n",
       "feat_eng_model                     0.743614      0.002021       0.664102   \n",
       "feat_eng_model_lemma_fix           0.744356      0.002107       0.664513   \n",
       "ensemble_rf_xgb                    0.779000      0.002740       0.697794   \n",
       "cos_sim_tfidf_model                0.729511      0.001216       0.661680   \n",
       "feat_eng_model_lemma_clean         0.763927      0.002404       0.676166   \n",
       "rf_feat_eng_model_lemma_clean      0.783667      0.002260       0.708853   \n",
       "ensemble_rf_xgb_cos_sim            0.738700      0.007359       0.661290   \n",
       "\n",
       "                               std_precision  avg_recall  std_recall  \\\n",
       "mvp (tf-idf, nmf(5), xgboost)       0.000461    0.385736    0.002493   \n",
       "mvp (+ lemma)                       0.003057    0.387424    0.003230   \n",
       "cos_sim_model                       0.002578    0.446336    0.002215   \n",
       "feat_eng_model                      0.003502    0.618400    0.001553   \n",
       "feat_eng_model_lemma_fix            0.004333    0.621357    0.000901   \n",
       "ensemble_rf_xgb                     0.004357    0.708157    0.001912   \n",
       "cos_sim_tfidf_model                 0.002219    0.547188    0.001744   \n",
       "feat_eng_model_lemma_clean          0.003904    0.692113    0.001128   \n",
       "rf_feat_eng_model_lemma_clean       0.003681    0.702725    0.001666   \n",
       "ensemble_rf_xgb_cos_sim             0.010948    0.612827    0.009669   \n",
       "\n",
       "                                 avg_f1    std_f1   avg_auc   std_auc  \\\n",
       "mvp (tf-idf, nmf(5), xgboost)  0.487325  0.001983  0.740593  0.001647   \n",
       "mvp (+ lemma)                  0.485464  0.002485  0.738037  0.001362   \n",
       "cos_sim_model                  0.532120  0.001306  0.746769  0.001279   \n",
       "feat_eng_model                 0.640434  0.002281  0.821070  0.001428   \n",
       "feat_eng_model_lemma_fix       0.642201  0.001609  0.822197  0.001710   \n",
       "ensemble_rf_xgb                0.702935  0.003148  0.863334  0.001438   \n",
       "cos_sim_tfidf_model            0.599010  0.001703  0.800271  0.001291   \n",
       "feat_eng_model_lemma_clean     0.684044  0.002549  0.846923  0.001643   \n",
       "rf_feat_eng_model_lemma_clean  0.705774  0.002658  0.868202  0.001148   \n",
       "ensemble_rf_xgb_cos_sim        0.636128  0.009994  0.819987  0.005193   \n",
       "\n",
       "                               avg_log_loss  std_log_loss  \n",
       "mvp (tf-idf, nmf(5), xgboost)      0.568958      0.001288  \n",
       "mvp (+ lemma)                      0.572483      0.000815  \n",
       "cos_sim_model                      0.565250      0.000963  \n",
       "feat_eng_model                     0.489465      0.001141  \n",
       "feat_eng_model_lemma_fix           0.488131      0.001342  \n",
       "ensemble_rf_xgb                    0.441784      0.001107  \n",
       "cos_sim_tfidf_model                0.512085      0.001299  \n",
       "feat_eng_model_lemma_clean         0.456929      0.001410  \n",
       "rf_feat_eng_model_lemma_clean      0.436197      0.000640  \n",
       "ensemble_rf_xgb_cos_sim            0.493703      0.003901  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = utils.load('results')\n",
    "\n",
    "results_df = results_df.drop(index='ensemble_rf_xgb_cos_sim', errors='ignore')\n",
    "results_df = results_df.append(utils.log_scores(cv, 'ensemble_rf_xgb_cos_sim'))\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save(results_df, 'results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Wow! The feature engineering shows a significant jump in AUC from 0.8 to 0.82."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_we...ate=42, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=True, subsample=1))],\n",
       "         flatten_transform=None, n_jobs=None, voting='soft', weights=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc = VotingClassifier([('rf', rf), ('xgb', xgb)], voting='soft')\n",
    "vc.fit(X_transform, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gt</th>\n",
       "      <th>prob</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.271811</td>\n",
       "      <td>-0.271811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.331749</td>\n",
       "      <td>-0.331749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.158016</td>\n",
       "      <td>-0.158016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>-0.000237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.034689</td>\n",
       "      <td>-0.034689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gt      prob      diff\n",
       "0   0  0.271811 -0.271811\n",
       "1   0  0.331749 -0.331749\n",
       "2   0  0.158016 -0.158016\n",
       "3   0  0.000237 -0.000237\n",
       "4   0  0.034689 -0.034689"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_probs = vc.predict_proba(X_transform)[:, 1]\n",
    "class_errors_df = utils.ground_truth_analysis(y_train, y_probs)\n",
    "class_errors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_pipe = Pipeline(\n",
    "    [\n",
    "        ('stack', FunctionTransformer(utils.stack_questions, validate=False)),\n",
    "        ('clean', FunctionTransformer(utils.clean_questions, validate=False)),\n",
    "        ('lemma', FunctionTransformer(utils.apply_lemma, validate=False)),\n",
    "    ]\n",
    ")\n",
    "X_train_lemma = lemma_pipe.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top false negative errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_idx = class_errors_df.sort_values('diff', ascending = False).head().index\n",
    "for idx in fn_idx:\n",
    "    print('Prob:', y_probs[idx])\n",
    "    print()\n",
    "    print(X_train.iloc[idx].question1)\n",
    "    print(X_train.iloc[idx].question2)\n",
    "    print()\n",
    "    print('Lemma--------')\n",
    "    print()\n",
    "    print(X_train_lemma[idx*2])\n",
    "    print(X_train_lemma[idx*2+1])\n",
    "    print()\n",
    "    print('Feature Space------')\n",
    "    print(X_transform[idx])\n",
    "    print('-------------------------------------------')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top false positive errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_idx = class_errors_df.sort_values('diff').head().index\n",
    "for idx in fn_idx:\n",
    "    print('Prob:', y_probs[idx])\n",
    "    print()\n",
    "    print(X_train.iloc[idx].question1)\n",
    "    print(X_train.iloc[idx].question2)\n",
    "    print()\n",
    "    print('Lemma--------')\n",
    "    print()\n",
    "    print(X_train_lemma[idx*2])\n",
    "    print(X_train_lemma[idx*2+1])\n",
    "    print()\n",
    "    print('Feature Space------')\n",
    "    print(X_transform[idx])\n",
    "    print('-------------------------------------------')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. The false negative errors appear to be related to numbers. \n",
    "  * Either stop removing numebers in the clean text proces, or create another set of features.\n",
    "2. The false positives seem very tricky. Found the Facebook InferSent model which embeds sentences to 4096 dimension.\n",
    "  * Could see if the distances between some of the false positive examples is different in this space.\n",
    "3. Could look at alignments of the two questions. (I think nltk)\n",
    "4. Add different word embeddings to get a different nuance, or even use the vector only data set from Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_cos_proba = nmf_cos.predict_proba(X_nmf_cos_feat)[:, 1]\n",
    "xgb_proba = xgb.predict_proba(X_feat_eng_transform)[:, 1]\n",
    "rf_proba = rf.predict_proba(X_feat_eng_transform)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39567994, 0.34479889, 0.17470645, 0.00103656, 0.07662025,\n",
       "       0.45168266, 0.29587881, 0.14138014, 0.0013542 , 0.68504003])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_cos_proba = nmf_cos_proba * cos_sim_weight\n",
    "xgb_proba = xgb_proba * xgb_weight\n",
    "rf_proba = rf_proba * rf_weight\n",
    "ensemble_proba = nmf_cos_proba + xgb_proba + rf_proba\n",
    "ensemble_proba[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble: 0.9846300420170292\n",
      "Nmf_cos: 0.8056886449194024\n",
      "XGB: 0.8540229521257536\n",
      "RF: 0.9999764283543164\n"
     ]
    }
   ],
   "source": [
    "print(f'Ensemble: {metrics.roc_auc_score(y_train, ensemble_proba)}')\n",
    "print(f'Nmf_cos: {metrics.roc_auc_score(y_train, nmf_cos_proba)}')\n",
    "print(f'XGB: {metrics.roc_auc_score(y_train, xgb_proba)}')\n",
    "print(f'RF: {metrics.roc_auc_score(y_train, rf_proba)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble: 0.32102219836966456\n",
      "Nmf_cos: 0.5069449937264294\n",
      "XGB: 0.4495672382949828\n",
      "RF: 0.12308486086868513\n"
     ]
    }
   ],
   "source": [
    "print(f'Ensemble: {metrics.log_loss(y_train, ensemble_proba)}')\n",
    "print(f'Nmf_cos: {metrics.log_loss(y_train, nmf_cos_proba)}')\n",
    "print(f'XGB: {metrics.log_loss(y_train, xgb_proba)}')\n",
    "print(f'RF: {metrics.log_loss(y_train, rf_proba)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
